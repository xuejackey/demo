{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (19.3.1)\n",
      "Requirement already satisfied: segtok in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (1.5.7)\n",
      "Requirement already satisfied: regex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from segtok) (2019.12.20)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip3 install segtok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Union\n",
    "\n",
    "import torch\n",
    "import logging\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "from segtok.segmenter import split_single\n",
    "from segtok.tokenizer import split_contractions\n",
    "from segtok.tokenizer import word_tokenizer\n",
    "\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Dictionary:\n",
    "    \"\"\"\n",
    "    This class holds a dictionary that maps strings to IDs, used to generate one-hot encodings of strings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, add_unk=True):\n",
    "        # init dictionaries\n",
    "        self.item2idx: Dict[str, int] = {}\n",
    "        self.idx2item: List[str] = []\n",
    "\n",
    "        # in order to deal with unknown tokens, add <unk>\n",
    "        if add_unk:\n",
    "            self.add_item('<unk>')\n",
    "\n",
    "    def add_item(self, item: str) -> int:\n",
    "        \"\"\"\n",
    "        add string - if already in dictionary returns its ID. if not in dictionary, it will get a new ID.\n",
    "        :param item: a string for which to assign an id\n",
    "        :return: ID of string\n",
    "        \"\"\"\n",
    "        item = item.encode('utf-8')\n",
    "        if item not in self.item2idx:\n",
    "            self.idx2item.append(item)\n",
    "            self.item2idx[item] = len(self.idx2item) - 1\n",
    "        return self.item2idx[item]\n",
    "\n",
    "    def get_idx_for_item(self, item: str) -> int:\n",
    "        \"\"\"\n",
    "        returns the ID of the string, otherwise 0\n",
    "        :param item: string for which ID is requested\n",
    "        :return: ID of string, otherwise 0\n",
    "        \"\"\"\n",
    "        item = item.encode('utf-8')\n",
    "        if item in self.item2idx.keys():\n",
    "            return self.item2idx[item]\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def get_items(self) -> List[str]:\n",
    "        items = []\n",
    "        for item in self.idx2item:\n",
    "            items.append(item.decode('UTF-8'))\n",
    "        return items\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.idx2item)\n",
    "\n",
    "    def get_item_for_index(self, idx):\n",
    "        return self.idx2item[idx].decode('UTF-8')\n",
    "\n",
    "    def save(self, savefile):\n",
    "        import pickle\n",
    "        with open(savefile, 'wb') as f:\n",
    "            mappings = {\n",
    "                'idx2item': self.idx2item,\n",
    "                'item2idx': self.item2idx\n",
    "            }\n",
    "            pickle.dump(mappings, f)\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_file(cls, filename: str):\n",
    "        import pickle\n",
    "        dictionary: Dictionary = Dictionary()\n",
    "        with open(filename, 'rb') as f:\n",
    "            mappings = pickle.load(f, encoding='latin1')\n",
    "            idx2item = mappings['idx2item']\n",
    "            item2idx = mappings['item2idx']\n",
    "            dictionary.item2idx = item2idx\n",
    "            dictionary.idx2item = idx2item\n",
    "        return dictionary\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, name: str):\n",
    "        from flair.file_utils import cached_path\n",
    "        if name == 'chars' or name == 'common-chars':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models/common_characters'\n",
    "            char_dict = cached_path(base_path, cache_dir='datasets')\n",
    "            return Dictionary.load_from_file(char_dict)\n",
    "\n",
    "        return Dictionary.load_from_file(name)\n",
    "\n",
    "\n",
    "class Label:\n",
    "    \"\"\"\n",
    "    This class represents a label of a sentence. Each label has a value and optionally a confidence score. The\n",
    "    score needs to be between 0.0 and 1.0. Default value for the score is 1.0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, value: str, score: float = 1.0):\n",
    "        self.value = value\n",
    "        self.score = score\n",
    "        super().__init__()\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self._value\n",
    "\n",
    "    @value.setter\n",
    "    def value(self, value):\n",
    "        if not value and value != '':\n",
    "            raise ValueError('Incorrect label value provided. Label value needs to be set.')\n",
    "        else:\n",
    "            self._value = value\n",
    "\n",
    "    @property\n",
    "    def score(self):\n",
    "        return self._score\n",
    "\n",
    "    @score.setter\n",
    "    def score(self, score):\n",
    "        if 0.0 <= score <= 1.0:\n",
    "            self._score = score\n",
    "        else:\n",
    "            self._score = 1.0\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'value': self.value,\n",
    "            'confidence': self.score\n",
    "        }\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"{} ({})\".format(self._value, self._score)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"{} ({})\".format(self._value, self._score)\n",
    "\n",
    "\n",
    "class Token:\n",
    "    \"\"\"\n",
    "    This class represents one word in a tokenized sentence. Each token may have any number of tags. It may also point\n",
    "    to its head in a dependency tree.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 text: str,\n",
    "                 sem: str = None,\n",
    "                 idx: int = None,\n",
    "                 head_id: int = None,\n",
    "                 whitespace_after: bool = True,\n",
    "                 start_position: int = None\n",
    "                 ):\n",
    "        self.text: str = text\n",
    "        self.sem: str = sem \n",
    "        self.idx: int = idx\n",
    "        self.head_id: int = head_id\n",
    "        self.whitespace_after: bool = whitespace_after\n",
    "\n",
    "        self.start_pos = start_position\n",
    "        self.end_pos = start_position + len(text) if start_position is not None else None\n",
    "\n",
    "        self.sentence: Sentence = None\n",
    "        self._embeddings: Dict = {}\n",
    "        self.tags: Dict[str, Label] = {}\n",
    "\n",
    "    def add_tag(self, tag_type: str, tag_value: str, confidence=1.0):\n",
    "        tag = Label(tag_value, confidence)\n",
    "        self.tags[tag_type] = tag\n",
    "\n",
    "    def get_tag(self, tag_type: str) -> Label:\n",
    "        if tag_type in self.tags: return self.tags[tag_type]\n",
    "        return Label('')\n",
    "    def set_sem(self, sem: str):\n",
    "        self.sem = sem\n",
    "    \n",
    "\n",
    "    def get_head(self):\n",
    "        return self.sentence.get_token(self.head_id)\n",
    "\n",
    "    def set_embedding(self, name: str, vector: torch.autograd.Variable):\n",
    "        self._embeddings[name] = vector.cpu()\n",
    "\n",
    "    def clear_embeddings(self):\n",
    "        self._embeddings: Dict = {}\n",
    "\n",
    "    def get_embedding(self) -> torch.FloatTensor:\n",
    "\n",
    "        embeddings = [self._embeddings[embed] for embed in sorted(self._embeddings.keys())]\n",
    "\n",
    "        if embeddings:\n",
    "            return torch.cat(embeddings, dim=0)\n",
    "\n",
    "        return torch.FloatTensor()\n",
    "\n",
    "    @property\n",
    "    def start_position(self) -> int:\n",
    "        return self.start_pos\n",
    "\n",
    "    @property\n",
    "    def end_position(self) -> int:\n",
    "        return self.end_pos\n",
    "\n",
    "    @property\n",
    "    def embedding(self):\n",
    "        return self.get_embedding()\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return 'Token: {} {}'.format(self.idx, self.text) if self.idx is not None else 'Token: {}'.format(self.text)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return 'Token: {} {}'.format(self.idx, self.text) if self.idx is not None else 'Token: {}'.format(self.text)\n",
    "\n",
    "\n",
    "class Span:\n",
    "    \"\"\"\n",
    "    This class represents one textual span consisting of Tokens. A span may have a tag.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokens: List[Token], tag: str = None, score=1.):\n",
    "        self.tokens = tokens\n",
    "        self.tag = tag\n",
    "        self.score = score\n",
    "        self.start_pos = None\n",
    "        self.end_pos = None\n",
    "\n",
    "        if tokens:\n",
    "            self.start_pos = tokens[0].start_position\n",
    "            self.end_pos = tokens[len(tokens) - 1].end_position\n",
    "\n",
    "    @property\n",
    "    def text(self) -> str:\n",
    "        return ' '.join([t.text for t in self.tokens])\n",
    "\n",
    "    def to_original_text(self) -> str:\n",
    "        str = ''\n",
    "        pos = self.tokens[0].start_pos\n",
    "        for t in self.tokens:\n",
    "            while t.start_pos != pos:\n",
    "                str += ' '\n",
    "                pos += 1\n",
    "\n",
    "            str += t.text\n",
    "            pos += len(t.text)\n",
    "\n",
    "        return str\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            'text': self.to_original_text(),\n",
    "            'start_pos': self.start_pos,\n",
    "            'end_pos': self.end_pos,\n",
    "            'type': self.tag,\n",
    "            'confidence': self.score\n",
    "        }\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        ids = ','.join([str(t.idx) for t in self.tokens])\n",
    "        return '{}-span [{}]: \"{}\"'.format(self.tag, ids, self.text) \\\n",
    "            if self.tag is not None else 'span [{}]: \"{}\"'.format(ids, self.text)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        ids = ','.join([str(t.idx) for t in self.tokens])\n",
    "        return '<{}-span ({}): \"{}\">'.format(self.tag, ids, self.text) \\\n",
    "            if self.tag is not None else '<span ({}): \"{}\">'.format(ids, self.text)\n",
    "\n",
    "\n",
    "class Sentence:\n",
    "    \"\"\"\n",
    "    A Sentence is a list of Tokens and is used to represent a sentence or text fragment.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text: str = None, use_tokenizer: bool = False, labels: Union[List[Label], List[str]] = None):\n",
    "\n",
    "        super(Sentence, self).__init__()\n",
    "\n",
    "        self.tokens: List[Token] = []\n",
    "\n",
    "        self.labels: List[Label] = []\n",
    "        if labels is not None: self.add_labels(labels)\n",
    "\n",
    "        self._embeddings: Dict = {}\n",
    "\n",
    "        # if text is passed, instantiate sentence with tokens (words)\n",
    "        if text is not None:\n",
    "\n",
    "            # tokenize the text first if option selected\n",
    "            if use_tokenizer:\n",
    "\n",
    "                # use segtok for tokenization\n",
    "                tokens = []\n",
    "                sentences = split_single(text)\n",
    "                for sentence in sentences:\n",
    "                    contractions = split_contractions(word_tokenizer(sentence))\n",
    "                    tokens.extend(contractions)\n",
    "\n",
    "                # determine offsets for whitespace_after field\n",
    "                index = text.index\n",
    "                running_offset = 0\n",
    "                last_word_offset = -1\n",
    "                last_token = None\n",
    "                for word in tokens:\n",
    "                    try:\n",
    "                        word_offset = index(word, running_offset)\n",
    "                        start_position = word_offset\n",
    "                    except:\n",
    "                        word_offset = last_word_offset + 1\n",
    "                        start_position = running_offset + 1 if running_offset > 0 else running_offset\n",
    "\n",
    "                    token = Token(word, start_position=start_position)\n",
    "                    self.add_token(token)\n",
    "\n",
    "                    if word_offset - 1 == last_word_offset and last_token is not None:\n",
    "                        last_token.whitespace_after = False\n",
    "\n",
    "                    word_len = len(word)\n",
    "                    running_offset = word_offset + word_len\n",
    "                    last_word_offset = running_offset - 1\n",
    "                    last_token = token\n",
    "\n",
    "            # otherwise assumes whitespace tokenized text\n",
    "            else:\n",
    "                # add each word in tokenized string as Token object to Sentence\n",
    "                offset = 0\n",
    "                for word in text.split(' '):\n",
    "                    if word:\n",
    "                        try:\n",
    "                            word_offset = text.index(word, offset)\n",
    "                        except:\n",
    "                            word_offset = offset\n",
    "\n",
    "                        token = Token(word, start_position=word_offset)\n",
    "                        self.add_token(token)\n",
    "                        offset += len(word) + 1\n",
    "\n",
    "    def get_token(self, token_id: int) -> Token:\n",
    "        for token in self.tokens:\n",
    "            if token.idx == token_id:\n",
    "                return token\n",
    "\n",
    "    def add_token(self, token: Token):\n",
    "        self.tokens.append(token)\n",
    "\n",
    "        # set token idx if not set\n",
    "        token.sentence = self\n",
    "        if token.idx is None:\n",
    "            token.idx = len(self.tokens)\n",
    "\n",
    "    def get_spans(self, tag_type: str, min_score=-1) -> List[Span]:\n",
    "\n",
    "        spans: List[Span] = []\n",
    "\n",
    "        current_span = []\n",
    "\n",
    "        tags = defaultdict(lambda: 0.0)\n",
    "\n",
    "        previous_tag_value: str = 'O'\n",
    "        for token in self:\n",
    "\n",
    "            tag: Label = token.get_tag(tag_type)\n",
    "            tag_value = tag.value\n",
    "\n",
    "            # non-set tags are OUT tags\n",
    "            if tag_value == '' or tag_value == 'O':\n",
    "                tag_value = 'O-'\n",
    "\n",
    "            # anything that is not a BIOES tag is a SINGLE tag\n",
    "            if tag_value[0:2] not in ['B-', 'I-', 'O-', 'E-', 'S-']:\n",
    "                tag_value = 'S-' + tag_value\n",
    "\n",
    "            # anything that is not OUT is IN\n",
    "            in_span = False\n",
    "            if tag_value[0:2] not in ['O-']:\n",
    "                in_span = True\n",
    "\n",
    "            # single and begin tags start a new span\n",
    "            starts_new_span = False\n",
    "            if tag_value[0:2] in ['B-', 'S-']:\n",
    "                starts_new_span = True\n",
    "\n",
    "            if previous_tag_value[0:2] in ['S-'] and previous_tag_value[2:] != tag_value[2:] and in_span:\n",
    "                starts_new_span = True\n",
    "\n",
    "            if (starts_new_span or not in_span) and len(current_span) > 0:\n",
    "                scores = [t.get_tag(tag_type).score for t in current_span]\n",
    "                span_score = sum(scores) / len(scores)\n",
    "                if span_score > min_score:\n",
    "                    spans.append(Span(\n",
    "                        current_span,\n",
    "                        tag=sorted(tags.items(), key=lambda k_v: k_v[1], reverse=True)[0][0],\n",
    "                        score=span_score)\n",
    "                    )\n",
    "                current_span = []\n",
    "                tags = defaultdict(lambda: 0.0)\n",
    "\n",
    "            if in_span:\n",
    "                current_span.append(token)\n",
    "                weight = 1.1 if starts_new_span else 1.0\n",
    "                tags[tag_value[2:]] += weight\n",
    "\n",
    "            # remember previous tag\n",
    "            previous_tag_value = tag_value\n",
    "\n",
    "        if len(current_span) > 0:\n",
    "            scores = [t.get_tag(tag_type).score for t in current_span]\n",
    "            span_score = sum(scores) / len(scores)\n",
    "            if span_score > min_score:\n",
    "                spans.append(Span(\n",
    "                    current_span,\n",
    "                    tag=sorted(tags.items(), key=lambda k_v: k_v[1], reverse=True)[0][0],\n",
    "                    score=span_score)\n",
    "                )\n",
    "\n",
    "        return spans\n",
    "\n",
    "    def add_label(self, label: Union[Label, str]):\n",
    "        if type(label) is Label:\n",
    "            self.labels.append(label)\n",
    "\n",
    "        elif type(label) is str:\n",
    "            self.labels.append(Label(label))\n",
    "\n",
    "    def add_labels(self, labels: Union[List[Label], List[str]]):\n",
    "        for label in labels:\n",
    "            self.add_label(label)\n",
    "\n",
    "    def get_label_names(self) -> List[str]:\n",
    "        return [label.value for label in self.labels]\n",
    "\n",
    "    @property\n",
    "    def embedding(self):\n",
    "        return self.get_embedding()\n",
    "\n",
    "    def set_embedding(self, name: str, vector):\n",
    "        self._embeddings[name] = vector.cpu()\n",
    "\n",
    "    def get_embedding(self) -> torch.autograd.Variable:\n",
    "        embeddings = []\n",
    "        for embed in sorted(self._embeddings.keys()):\n",
    "            embedding = self._embeddings[embed]\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "        if embeddings:\n",
    "            return torch.cat(embeddings, dim=0)\n",
    "\n",
    "        return torch.FloatTensor()\n",
    "\n",
    "    def clear_embeddings(self, also_clear_word_embeddings: bool = True):\n",
    "        self._embeddings: Dict = {}\n",
    "\n",
    "        if also_clear_word_embeddings:\n",
    "            for token in self:\n",
    "                token.clear_embeddings()\n",
    "\n",
    "    def cpu_embeddings(self):\n",
    "        for name, vector in self._embeddings.items():\n",
    "            self._embeddings[name] = vector.cpu()\n",
    "\n",
    "    def to_tagged_string(self, main_tag=None) -> str:\n",
    "        list = []\n",
    "        for token in self.tokens:\n",
    "            list.append(token.text)\n",
    "\n",
    "            tags: List[str] = []\n",
    "            for tag_type in token.tags.keys():\n",
    "\n",
    "                if main_tag is not None and main_tag != tag_type: continue\n",
    "\n",
    "                if token.get_tag(tag_type).value == '' or token.get_tag(tag_type).value == 'O': continue\n",
    "                tags.append(token.get_tag(tag_type).value)\n",
    "            all_tags = '<' + '/'.join(tags) + '>'\n",
    "            if all_tags != '<>':\n",
    "                list.append(all_tags)\n",
    "        return ' '.join(list)\n",
    "    def get_tags(self, main_tag=None) -> str:\n",
    "        list = []\n",
    "        output = []\n",
    "        for token in self.tokens:\n",
    "            list.append(token.text)\n",
    "\n",
    "            tags: List[str] = []\n",
    "            for tag_type in token.tags.keys():\n",
    "\n",
    "                if main_tag is not None and main_tag != tag_type:\n",
    "                        output.append(\"O\")\n",
    "                        continue\n",
    "\n",
    "                #if token.get_tag(tag_type).value == '' or token.get_tag(tag_type).value == 'O': continue\n",
    "                \n",
    "                tags.append(token.get_tag(tag_type).value)\n",
    "            output.append(tags[0])\n",
    "        return output \n",
    "    def to_offset_tags(self, sent_offset):\n",
    "        list = []\n",
    "        offset = 0\n",
    "        cur_entity_start_offset = -1\n",
    "        cur_entity_text = \"\"\n",
    "        for token in self.tokens:\n",
    "                start = offset\n",
    "                #for tag_type in token.tags.keys():\n",
    "                        #if token.get_tag(tag_type) == '' or token.get_tag(tag_type).value == 'O':print (token.text, 'O')\n",
    "                        #else:  print (token.text, token.get_tag(tag_type).value)\t\n",
    "                offset += len(token.text)\n",
    "                #print ('-----')\n",
    "                #print (token.text)\n",
    "                tag_type = \"ner\"\n",
    "                #print (\"tag:\"+token.get_tag(tag_type).value)\n",
    "                if token.get_tag(tag_type).value.startswith(\"E-\"):\n",
    "                        cur_entity_text += token.text\n",
    "                        entity_start_offset = sent_offset + cur_entity_start_offset\n",
    "                        entity_end_offset = sent_offset + offset\n",
    "                        entity_tag = token.get_tag(tag_type).value[2:]\n",
    "                        entity_txt = cur_entity_text\n",
    "                        list.append([entity_tag, str(entity_start_offset), str(entity_end_offset), str(entity_txt)])\n",
    "\t\t\t#list.append(entity_tag + \" \" + str(entity_start_offset) + \" \" + str(entity_end_offset) + \"\\t\"+str(entity_txt))\t\n",
    "                        cur_entity_text = \"\"\n",
    "                        cur_entity_start_offset = -1\n",
    "                if token.get_tag(tag_type).value.startswith(\"B-\"):\n",
    "                        cur_entity_text = token.text\n",
    "                        cur_entity_start_offset = offset - len(token.text)\n",
    "                        if token.whitespace_after:\n",
    "                                cur_entity_text += \" \"\n",
    "                if token.get_tag(tag_type).value.startswith(\"I-\"):\n",
    "                        cur_entity_text += token.text\n",
    "                        if token.whitespace_after:\n",
    "                                cur_entity_text += \" \"\n",
    "                if token.get_tag(tag_type).value.startswith(\"S-\"):\n",
    "                        entity_start_offset = sent_offset + offset - len(token.text)\n",
    "                        entity_end_offset = sent_offset + offset\n",
    "                        entity_tag = token.get_tag(tag_type).value[2:]\n",
    "                        entity_txt = token.text\n",
    "                        list.append([entity_tag, str(entity_start_offset), str(entity_end_offset), str(entity_txt)]) \n",
    "                        #list.append(entity_tag + \" \" + str(entity_start_offset) + \" \" + str(entity_end_offset) + \"\\t\"+str(entity_txt))\n",
    "                        cur_entity_text = \"\"\n",
    "                        cur_entity_start_offset = -1\t\t         \n",
    "                if token.whitespace_after:\n",
    "                        offset += 1\n",
    "                end = offset\n",
    "                #print (\"cur_entity:\"+cur_entity_text)\n",
    "        return list\t\t\n",
    "\t\t\t\n",
    "\n",
    "\n",
    "    def to_tokenized_string(self) -> str:\n",
    "        return ' '.join([t.text for t in self.tokens])\n",
    "\n",
    "    def to_plain_string(self):\n",
    "        plain = ''\n",
    "        for token in self.tokens:\n",
    "            plain += token.text\n",
    "            if token.whitespace_after: plain += ' '\n",
    "        return plain.rstrip()\n",
    "\n",
    "    def convert_tag_scheme(self, tag_type: str = 'ner', target_scheme: str = 'iob'):\n",
    "\n",
    "        tags: List[Label] = []\n",
    "        for token in self.tokens:\n",
    "            token: Token = token\n",
    "            tags.append(token.get_tag(tag_type))\n",
    "        print (\"tokens:\", self.tokens)\n",
    "        print (\"tags:\", tags)\n",
    "        if target_scheme == 'iob':\n",
    "            iob2(tags)\n",
    "\n",
    "        if target_scheme == 'iobes':\n",
    "            iob2(tags)\n",
    "            tags = iob_iobes(tags)\n",
    "\n",
    "        for index, tag in enumerate(tags):\n",
    "            self.tokens[index].add_tag(tag_type, tag)\n",
    "\n",
    "    def infer_space_after(self):\n",
    "        \"\"\"\n",
    "        Heuristics in case you wish to infer whitespace_after values for tokenized text. This is useful for some old NLP\n",
    "        tasks (such as CoNLL-03 and CoNLL-2000) that provide only tokenized data with no info of original whitespacing.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        last_token = None\n",
    "        quote_count: int = 0\n",
    "        # infer whitespace after field\n",
    "\n",
    "        for token in self.tokens:\n",
    "            if token.text == '\"':\n",
    "                quote_count += 1\n",
    "                if quote_count % 2 != 0:\n",
    "                    token.whitespace_after = False\n",
    "                elif last_token is not None:\n",
    "                    last_token.whitespace_after = False\n",
    "\n",
    "            if last_token is not None:\n",
    "\n",
    "                if token.text in ['.', ':', ',', ';', ')', 'n\\'t', '!', '?']:\n",
    "                    last_token.whitespace_after = False\n",
    "\n",
    "                if token.text.startswith('\\''):\n",
    "                    last_token.whitespace_after = False\n",
    "\n",
    "            if token.text in ['(']:\n",
    "                token.whitespace_after = False\n",
    "\n",
    "            last_token = token\n",
    "        return self\n",
    "\n",
    "    def to_original_text(self) -> str:\n",
    "        str = ''\n",
    "        pos = 0\n",
    "        for t in self.tokens:\n",
    "            while t.start_pos != pos:\n",
    "                str += ' '\n",
    "                pos += 1\n",
    "\n",
    "            str += t.text\n",
    "            pos += len(t.text)\n",
    "\n",
    "        return str\n",
    "\n",
    "    def to_dict(self, tag_type: str = None):\n",
    "        labels = []\n",
    "        entities = []\n",
    "\n",
    "        if tag_type:\n",
    "            entities = [span.to_dict() for span in self.get_spans(tag_type)]\n",
    "        if self.labels:\n",
    "            labels = [l.to_dict() for l in self.labels]\n",
    "\n",
    "        return {\n",
    "            'text': self.to_original_text(),\n",
    "            'labels': labels,\n",
    "            'entities': entities\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Token:\n",
    "        return self.tokens[idx]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.tokens)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Sentence: \"{}\" - {} Tokens'.format(' '.join([t.text for t in self.tokens]), len(self))\n",
    "\n",
    "    def __copy__(self):\n",
    "        s = Sentence()\n",
    "        for token in self.tokens:\n",
    "            nt = Token(token.text)\n",
    "            for tag_type in token.tags:\n",
    "                nt.add_tag(tag_type, token.get_tag(tag_type).value, token.get_tag(tag_type).score)\n",
    "\n",
    "            s.add_token(nt)\n",
    "        return s\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return 'Sentence: \"{}\" - {} Tokens'.format(' '.join([t.text for t in self.tokens]), len(self))\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.tokens)\n",
    "\n",
    "\n",
    "class TaggedCorpus:\n",
    "    def __init__(self, train: List[Sentence], dev: List[Sentence], test: List[Sentence]):\n",
    "        self.train: List[Sentence] = train\n",
    "        self.dev: List[Sentence] = dev\n",
    "        self.test: List[Sentence] = test\n",
    "\n",
    "    def downsample(self, percentage: float = 0.1, only_downsample_train=False):\n",
    "\n",
    "        self.train = self._downsample_to_proportion(self.train, percentage)\n",
    "        if not only_downsample_train:\n",
    "            self.dev = self._downsample_to_proportion(self.dev, percentage)\n",
    "            self.test = self._downsample_to_proportion(self.test, percentage)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def clear_embeddings(self):\n",
    "        for sentence in self.get_all_sentences():\n",
    "            for token in sentence.tokens:\n",
    "                token.clear_embeddings()\n",
    "\n",
    "    def get_all_sentences(self) -> List[Sentence]:\n",
    "        all_sentences: List[Sentence] = []\n",
    "        all_sentences.extend(self.train)\n",
    "        all_sentences.extend(self.dev)\n",
    "        all_sentences.extend(self.test)\n",
    "        return all_sentences\n",
    "\n",
    "    def make_tag_dictionary(self, tag_type: str) -> Dictionary:\n",
    "\n",
    "        # Make the tag dictionary\n",
    "        tag_dictionary: Dictionary = Dictionary()\n",
    "        tag_dictionary.add_item('O')\n",
    "        for sentence in self.get_all_sentences():\n",
    "            for token in sentence.tokens:\n",
    "                token: Token = token\n",
    "                tag_dictionary.add_item(token.get_tag(tag_type).value)\n",
    "        tag_dictionary.add_item('<START>')\n",
    "        tag_dictionary.add_item('<STOP>')\n",
    "        return tag_dictionary\n",
    "\n",
    "    def make_label_dictionary(self) -> Dictionary:\n",
    "        \"\"\"\n",
    "        Creates a dictionary of all labels assigned to the sentences in the corpus.\n",
    "        :return: dictionary of labels\n",
    "        \"\"\"\n",
    "\n",
    "        labels = set(self._get_all_label_names())\n",
    "\n",
    "        label_dictionary: Dictionary = Dictionary(add_unk=False)\n",
    "        for label in labels:\n",
    "            label_dictionary.add_item(label)\n",
    "\n",
    "        return label_dictionary\n",
    "\n",
    "    def make_vocab_dictionary(self, max_tokens=-1, min_freq=1) -> Dictionary:\n",
    "        \"\"\"\n",
    "        Creates a dictionary of all tokens contained in the corpus.\n",
    "        By defining `max_tokens` you can set the maximum number of tokens that should be contained in the dictionary.\n",
    "        If there are more than `max_tokens` tokens in the corpus, the most frequent tokens are added first.\n",
    "        If `min_freq` is set the a value greater than 1 only tokens occurring more than `min_freq` times are considered\n",
    "        to be added to the dictionary.\n",
    "        :param max_tokens: the maximum number of tokens that should be added to the dictionary (-1 = take all tokens)\n",
    "        :param min_freq: a token needs to occur at least `min_freq` times to be added to the dictionary (-1 = there is no limitation)\n",
    "        :return: dictionary of tokens\n",
    "        \"\"\"\n",
    "        tokens = self._get_most_common_tokens(max_tokens, min_freq)\n",
    "\n",
    "        vocab_dictionary: Dictionary = Dictionary()\n",
    "        for token in tokens:\n",
    "            vocab_dictionary.add_item(token)\n",
    "\n",
    "        return vocab_dictionary\n",
    "\n",
    "    def _get_most_common_tokens(self, max_tokens, min_freq) -> List[str]:\n",
    "        tokens_and_frequencies = Counter(self._get_all_tokens())\n",
    "        tokens_and_frequencies = tokens_and_frequencies.most_common()\n",
    "\n",
    "        tokens = []\n",
    "        for token, freq in tokens_and_frequencies:\n",
    "            if (min_freq != -1 and freq < min_freq) or (max_tokens != -1 and len(tokens) == max_tokens):\n",
    "                break\n",
    "            tokens.append(token)\n",
    "        return tokens\n",
    "\n",
    "    def _get_all_label_names(self) -> List[str]:\n",
    "        return [label.value for sent in self.train for label in sent.labels]\n",
    "\n",
    "    def _get_all_tokens(self) -> List[str]:\n",
    "        tokens = list(map((lambda s: s.tokens), self.train))\n",
    "        tokens = [token for sublist in tokens for token in sublist]\n",
    "        return list(map((lambda t: t.text), tokens))\n",
    "\n",
    "    def _downsample_to_proportion(self, list: List, proportion: float):\n",
    "\n",
    "        counter = 0.0\n",
    "        last_counter = None\n",
    "        downsampled: List = []\n",
    "\n",
    "        for item in list:\n",
    "            counter += proportion\n",
    "            if int(counter) != last_counter:\n",
    "                downsampled.append(item)\n",
    "                last_counter = int(counter)\n",
    "        return downsampled\n",
    "\n",
    "    def print_statistics(self):\n",
    "        \"\"\"\n",
    "        Print statistics about the class distribution (only labels of sentences are taken into account) and sentence\n",
    "        sizes.\n",
    "        \"\"\"\n",
    "        self._print_statistics_for(self.train, \"TRAIN\")\n",
    "        self._print_statistics_for(self.test, \"TEST\")\n",
    "        self._print_statistics_for(self.dev, \"DEV\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _print_statistics_for(sentences, name):\n",
    "        if len(sentences) == 0:\n",
    "            return\n",
    "\n",
    "        classes_to_count = TaggedCorpus._get_classes_to_count(sentences)\n",
    "        tokens_per_sentence = TaggedCorpus._get_tokens_per_sentence(sentences)\n",
    "\n",
    "        size_dict = {}\n",
    "        for l, c in classes_to_count.items():\n",
    "            size_dict[l] = c\n",
    "        size_dict['total'] = len(sentences)\n",
    "\n",
    "        stats = {\n",
    "            'dataset': name,\n",
    "            'number_of_documents': size_dict,\n",
    "            'number_of_tokens': {\n",
    "                'total': sum(tokens_per_sentence),\n",
    "                'min': min(tokens_per_sentence),\n",
    "                'max': max(tokens_per_sentence),\n",
    "                'avg': sum(tokens_per_sentence) / len(sentences)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        log.info(stats)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_tokens_per_sentence(sentences):\n",
    "        return list(map(lambda x: len(x.tokens), sentences))\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_classes_to_count(sentences):\n",
    "        classes_to_count = defaultdict(lambda: 0)\n",
    "        for sent in sentences:\n",
    "            for label in sent.labels:\n",
    "                classes_to_count[label.value] += 1\n",
    "        return classes_to_count\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return 'TaggedCorpus: %d train + %d dev + %d test sentences' % (len(self.train), len(self.dev), len(self.test))\n",
    "\n",
    "\n",
    "def iob2(tags):\n",
    "    \"\"\"\n",
    "    Check that tags have a valid IOB format.\n",
    "    Tags in IOB1 format are converted to IOB2.\n",
    "    \"\"\"\n",
    "    for i, tag in enumerate(tags):\n",
    "        # print(tag)\n",
    "        if tag.value == 'O':\n",
    "            continue\n",
    "        split = tag.value.split('-')\n",
    "        if len(split) != 2 or split[0] not in ['I', 'B']:\n",
    "            return False\n",
    "        if split[0] == 'B':\n",
    "            continue\n",
    "        elif i == 0 or tags[i - 1].value == 'O':  # conversion IOB1 to IOB2\n",
    "            tags[i].value = 'B' + tag.value[1:]\n",
    "        elif tags[i - 1].value[1:] == tag.value[1:]:\n",
    "            continue\n",
    "        else:  # conversion IOB1 to IOB2\n",
    "            tags[i].value = 'B' + tag.value[1:]\n",
    "    return True\n",
    "\n",
    "\n",
    "def iob_iobes(tags):\n",
    "    \"\"\"\n",
    "    IOB -> IOBES\n",
    "    \"\"\"\n",
    "    new_tags = []\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag.value == 'O':\n",
    "            new_tags.append(tag.value)\n",
    "        elif tag.value.split('-')[0] == 'B':\n",
    "            if i + 1 != len(tags) and \\\n",
    "                    tags[i + 1].value.split('-')[0] == 'I':\n",
    "                new_tags.append(tag.value)\n",
    "            else:\n",
    "                new_tags.append(tag.value.replace('B-', 'S-'))\n",
    "        elif tag.value.split('-')[0] == 'I':\n",
    "            if i + 1 < len(tags) and \\\n",
    "                    tags[i + 1].value.split('-')[0] == 'I':\n",
    "                new_tags.append(tag.value)\n",
    "            else:\n",
    "                new_tags.append(tag.value.replace('I-', 'E-'))\n",
    "        else:\n",
    "            print (tag)\n",
    "            raise Exception('Invalid IOB format!')\n",
    "    return new_tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utilities for working with the local dataset cache. Copied from AllenNLP\n",
    "\"\"\"\n",
    "\n",
    "from typing import Tuple\n",
    "import os\n",
    "import base64\n",
    "import logging\n",
    "import shutil\n",
    "import tempfile\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import requests\n",
    "\n",
    "# from allennlp.common.tqdm import Tqdm\n",
    "\n",
    "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
    "\n",
    "CACHE_ROOT = os.path.expanduser(os.path.join('~', '.flair'))\n",
    "\n",
    "\n",
    "def url_to_filename(url: str, etag: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Converts a url into a filename in a reversible way.\n",
    "    If `etag` is specified, add it on the end, separated by a period\n",
    "    (which necessarily won't appear in the base64-encoded filename).\n",
    "    Get rid of the quotes in the etag, since Windows doesn't like them.\n",
    "    \"\"\"\n",
    "    url_bytes = url.encode('utf-8')\n",
    "    b64_bytes = base64.b64encode(url_bytes)\n",
    "    decoded = b64_bytes.decode('utf-8')\n",
    "\n",
    "    if etag:\n",
    "        # Remove quotes from etag\n",
    "        etag = etag.replace('\"', '')\n",
    "        return f\"{decoded}.{etag}\"\n",
    "    else:\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def filename_to_url(filename: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Recovers the the url from the encoded filename. Returns it and the ETag\n",
    "    (which may be ``None``)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If there is an etag, it's everything after the first period\n",
    "        decoded, etag = filename.split(\".\", 1)\n",
    "    except ValueError:\n",
    "        # Otherwise, use None\n",
    "        decoded, etag = filename, None\n",
    "\n",
    "    filename_bytes = decoded.encode('utf-8')\n",
    "    url_bytes = base64.b64decode(filename_bytes)\n",
    "    return url_bytes.decode('utf-8'), etag\n",
    "\n",
    "\n",
    "def cached_path(url_or_filename: str, cache_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    Given something that might be a URL (or might be a local path),\n",
    "    determine which. If it's a URL, download the file and cache it, and\n",
    "    return the path to the cached file. If it's already a local path,\n",
    "    make sure the file exists and then return the path.\n",
    "    \"\"\"\n",
    "    dataset_cache = os.path.join(CACHE_ROOT, cache_dir)\n",
    "\n",
    "    parsed = urlparse(url_or_filename)\n",
    "\n",
    "    if parsed.scheme in ('http', 'https'):\n",
    "        # URL, so get it from the cache (downloading if necessary)\n",
    "        return get_from_cache(url_or_filename, dataset_cache)\n",
    "    elif parsed.scheme == '' and os.path.exists(url_or_filename):\n",
    "        # File, and it exists.\n",
    "        return url_or_filename\n",
    "    elif parsed.scheme == '':\n",
    "        # File, but it doesn't exist.\n",
    "        raise FileNotFoundError(\"file {} not found\".format(url_or_filename))\n",
    "    else:\n",
    "        # Something unknown\n",
    "        raise ValueError(\"unable to parse {} as a URL or as a local path\".format(url_or_filename))\n",
    "\n",
    "\n",
    "# TODO(joelgrus): do we want to do checksums or anything like that?\n",
    "def get_from_cache(url: str, cache_dir: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Given a URL, look for the corresponding dataset in the local cache.\n",
    "    If it's not there, download it. Then return the path to the cached file.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    filename = re.sub(r'.+/', '', url)\n",
    "    # get cache path to put the file\n",
    "    cache_path = os.path.join(cache_dir, filename)\n",
    "    if os.path.exists(cache_path):\n",
    "        return cache_path\n",
    "\n",
    "    # make HEAD request to check ETag\n",
    "    response = requests.head(url)\n",
    "    if response.status_code != 200:\n",
    "        raise IOError(\"HEAD request failed for url {}\".format(url))\n",
    "\n",
    "    # add ETag to filename if it exists\n",
    "    # etag = response.headers.get(\"ETag\")\n",
    "\n",
    "    if not os.path.exists(cache_path):\n",
    "        # Download to temporary file, then copy to cache dir once finished.\n",
    "        # Otherwise you get corrupt cache entries if the download gets interrupted.\n",
    "        _, temp_filename = tempfile.mkstemp()\n",
    "        logger.info(\"%s not found in cache, downloading to %s\", url, temp_filename)\n",
    "\n",
    "        # GET file object\n",
    "        req = requests.get(url, stream=True)\n",
    "        content_length = req.headers.get('Content-Length')\n",
    "        total = int(content_length) if content_length is not None else None\n",
    "        progress = Tqdm.tqdm(unit=\"B\", total=total)\n",
    "        with open(temp_filename, 'wb') as temp_file:\n",
    "            for chunk in req.iter_content(chunk_size=1024):\n",
    "                if chunk: # protocol_filter out keep-alive new chunks\n",
    "                    progress.update(len(chunk))\n",
    "                    temp_file.write(chunk)\n",
    "\n",
    "        progress.close()\n",
    "\n",
    "        logger.info(\"copying %s to cache at %s\", temp_filename, cache_path)\n",
    "        shutil.copyfile(temp_filename, cache_path)\n",
    "        logger.info(\"removing temp file %s\", temp_filename)\n",
    "        os.remove(temp_filename)\n",
    "\n",
    "    return cache_path\n",
    "\n",
    "\n",
    "from tqdm import tqdm as _tqdm\n",
    "\n",
    "\n",
    "class Tqdm:\n",
    "    # These defaults are the same as the argument defaults in tqdm.\n",
    "    default_mininterval: float = 0.1\n",
    "\n",
    "    @staticmethod\n",
    "    def set_default_mininterval(value: float) -> None:\n",
    "        Tqdm.default_mininterval = value\n",
    "\n",
    "    @staticmethod\n",
    "    def set_slower_interval(use_slower_interval: bool) -> None:\n",
    "        \"\"\"\n",
    "        If ``use_slower_interval`` is ``True``, we will dramatically slow down ``tqdm's`` default\n",
    "        output rate.  ``tqdm's`` default output rate is great for interactively watching progress,\n",
    "        but it is not great for log files.  You might want to set this if you are primarily going\n",
    "        to be looking at output through log files, not the terminal.\n",
    "        \"\"\"\n",
    "        if use_slower_interval:\n",
    "            Tqdm.default_mininterval = 10.0\n",
    "        else:\n",
    "            Tqdm.default_mininterval = 0.1\n",
    "\n",
    "    @staticmethod\n",
    "    def tqdm(*args, **kwargs):\n",
    "        new_kwargs = {\n",
    "                'mininterval': Tqdm.default_mininterval,\n",
    "                **kwargs\n",
    "        }\n",
    "\n",
    "        return _tqdm(*args, **new_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flair in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.4.4)\n",
      "Requirement already satisfied: pymongo in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (3.10.0)\n",
      "Requirement already satisfied: ipython-genutils==0.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (0.2.0)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (0.4.0a0+6b959ee)\n",
      "Requirement already satisfied: pytest>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (5.3.2)\n",
      "Requirement already satisfied: bpemb>=0.2.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (0.3.0)\n",
      "Requirement already satisfied: mpld3==0.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (0.3)\n",
      "Requirement already satisfied: gensim>=3.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (3.8.1)\n",
      "Requirement already satisfied: tiny-tokenizer[all] in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (3.1.0)\n",
      "Requirement already satisfied: segtok>=1.5.7 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (1.5.7)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (3.0.3)\n",
      "Requirement already satisfied: torch>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (1.2.0)\n",
      "Requirement already satisfied: transformers>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (2.3.0)\n",
      "Requirement already satisfied: deprecated>=1.2.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (1.2.7)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (0.8.6)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.20 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (1.23)\n",
      "Requirement already satisfied: langdetect in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (1.0.7)\n",
      "Requirement already satisfied: sklearn in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (0.0)\n",
      "Requirement already satisfied: ipython==7.6.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (7.6.1)\n",
      "Requirement already satisfied: hyperopt>=0.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (0.2.2)\n",
      "Requirement already satisfied: sqlitedict>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (1.6.0)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (4.36.1)\n",
      "Requirement already satisfied: regex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (2019.12.20)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->flair) (1.15.4)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->flair) (1.11.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->flair) (5.2.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytest>=3.6.4->flair) (0.13.1)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytest>=3.6.4->flair) (4.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytest>=3.6.4->flair) (1.3.0)\n",
      "Requirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytest>=3.6.4->flair) (0.1.7)\n",
      "Requirement already satisfied: py>=1.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytest>=3.6.4->flair) (1.5.3)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytest>=3.6.4->flair) (17.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytest>=3.6.4->flair) (18.1.0)\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from bpemb>=0.2.9->flair) (0.1.85)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from bpemb>=0.2.9->flair) (2.20.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from gensim>=3.4.0->flair) (1.9.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from gensim>=3.4.0->flair) (1.1.0)\n",
      "Requirement already satisfied: SudachiDict-core; extra == \"all\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tiny-tokenizer[all]->flair) (0.0.0)\n",
      "Requirement already satisfied: janome; extra == \"all\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tiny-tokenizer[all]->flair) (0.3.10)\n",
      "Requirement already satisfied: natto-py; extra == \"all\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tiny-tokenizer[all]->flair) (0.9.2)\n",
      "Requirement already satisfied: SudachiPy; extra == \"all\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tiny-tokenizer[all]->flair) (0.4.2)\n",
      "Requirement already satisfied: kytea; extra == \"all\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tiny-tokenizer[all]->flair) (0.1.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib>=2.2.3->flair) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib>=2.2.3->flair) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib>=2.2.3->flair) (2.7.3)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=2.0.0->flair) (1.10.19)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=2.0.0->flair) (0.0.35)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from deprecated>=1.2.4->flair) (1.10.11)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sklearn->flair) (0.20.3)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython==7.6.1->flair) (4.5.0)\n",
      "Requirement already satisfied: backcall in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython==7.6.1->flair) (0.1.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython==7.6.1->flair) (4.3.2)\n",
      "Requirement already satisfied: decorator in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython==7.6.1->flair) (4.3.0)\n",
      "Requirement already satisfied: pickleshare in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython==7.6.1->flair) (0.7.4)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython==7.6.1->flair) (2.0.10)\n",
      "Requirement already satisfied: pygments in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython==7.6.1->flair) (2.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython==7.6.1->flair) (39.1.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython==7.6.1->flair) (0.12.0)\n",
      "Requirement already satisfied: networkx==2.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from hyperopt>=0.1.1->flair) (2.2)\n",
      "Requirement already satisfied: cloudpickle in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from hyperopt>=0.1.1->flair) (0.5.3)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from hyperopt>=0.1.1->flair) (0.18.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=3.6.4->flair) (0.6.0)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->bpemb>=0.2.9->flair) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->bpemb>=0.2.9->flair) (2019.9.11)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->bpemb>=0.2.9->flair) (3.0.4)\n",
      "Requirement already satisfied: boto>=2.32 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim>=3.4.0->flair) (2.48.0)\n",
      "Requirement already satisfied: cffi in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from natto-py; extra == \"all\"->tiny-tokenizer[all]->flair) (1.11.5)\n",
      "Requirement already satisfied: dartsclone~=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from SudachiPy; extra == \"all\"->tiny-tokenizer[all]->flair) (0.6)\n",
      "Requirement already satisfied: sortedcontainers~=2.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from SudachiPy; extra == \"all\"->tiny-tokenizer[all]->flair) (2.1.0)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.19 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->transformers>=2.0.0->flair) (1.13.19)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->transformers>=2.0.0->flair) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->transformers>=2.0.0->flair) (0.9.4)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers>=2.0.0->flair) (0.14.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers>=2.0.0->flair) (6.7)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython==7.6.1->flair) (0.5.2)\n",
      "Requirement already satisfied: parso>=0.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from jedi>=0.10->ipython==7.6.1->flair) (0.2.0)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from cffi->natto-py; extra == \"all\"->tiny-tokenizer[all]->flair) (2.18)\n",
      "Requirement already satisfied: Cython in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from dartsclone~=0.6.0->SudachiPy; extra == \"all\"->tiny-tokenizer[all]->flair) (0.28.2)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.19->boto3->transformers>=2.0.0->flair) (0.14)\n"
     ]
    }
   ],
   "source": [
    "#!pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "\n",
    "\n",
    "class LockedDropout(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of locked (or variational) dropout. Randomly drops out entire parameters in embedding space.\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(LockedDropout, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training or not self.dropout_rate:\n",
    "            return x\n",
    "\n",
    "        m = x.data.new(1, x.size(1), x.size(2)).bernoulli_(1 - self.dropout_rate)\n",
    "        mask = torch.autograd.Variable(m, requires_grad=False) / (1 - self.dropout_rate)\n",
    "        mask = mask.expand_as(x)\n",
    "        return mask * x\n",
    "\n",
    "\n",
    "class WordDropout(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of word dropout. Randomly drops out entire words (or characters) in embedding space.\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_rate=0.05):\n",
    "        super(WordDropout, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training or not self.dropout_rate:\n",
    "            return x\n",
    "\n",
    "        m = x.data.new(x.size(0), 1, 1).bernoulli_(1 - self.dropout_rate)\n",
    "        mask = torch.autograd.Variable(m, requires_grad=False)\n",
    "        mask = mask.expand_as(x)\n",
    "        return mask * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "# from flair.data import Dictionary, Sentence\n",
    "from functools import reduce\n",
    "\n",
    "MICRO_AVG_METRIC = 'MICRO_AVG'\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Metric(object):\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "        self._tps = defaultdict(int)\n",
    "        self._fps = defaultdict(int)\n",
    "        self._tns = defaultdict(int)\n",
    "        self._fns = defaultdict(int)\n",
    "\n",
    "    def tp(self, class_name=None):\n",
    "        self._tps[class_name] += 1\n",
    "\n",
    "    def tn(self, class_name=None):\n",
    "        self._tns[class_name] += 1\n",
    "\n",
    "    def fp(self, class_name=None):\n",
    "        self._fps[class_name] += 1\n",
    "\n",
    "    def fn(self, class_name=None):\n",
    "        self._fns[class_name] += 1\n",
    "\n",
    "    def get_tp(self, class_name=None):\n",
    "        return self._tps[class_name]\n",
    "\n",
    "    def get_tn(self, class_name=None):\n",
    "        return self._tns[class_name]\n",
    "\n",
    "    def get_fp(self, class_name=None):\n",
    "        return self._fps[class_name]\n",
    "\n",
    "    def get_fn(self, class_name=None):\n",
    "        return self._fns[class_name]\n",
    "\n",
    "    def precision(self, class_name=None):\n",
    "        if self._tps[class_name] + self._fps[class_name] > 0:\n",
    "            return round(self._tps[class_name] / (self._tps[class_name] + self._fps[class_name]), 4)\n",
    "        return 0.0\n",
    "\n",
    "    def recall(self, class_name=None):\n",
    "        if self._tps[class_name] + self._fns[class_name] > 0:\n",
    "            return round(self._tps[class_name] / (self._tps[class_name] + self._fns[class_name]), 4)\n",
    "        return 0.0\n",
    "\n",
    "    def f_score(self, class_name=None):\n",
    "        if self.precision(class_name) + self.recall(class_name) > 0:\n",
    "            return round(2 * (self.precision(class_name) * self.recall(class_name))\n",
    "                         / (self.precision(class_name) + self.recall(class_name)), 4)\n",
    "        return 0.0\n",
    "\n",
    "    def micro_avg_f_score(self):\n",
    "        all_tps = sum([self.tp(class_name) for class_name in self.get_classes()])\n",
    "        all_fps = sum([self.fp(class_name) for class_name in self.get_classes()])\n",
    "        all_fns = sum([self.fn(class_name) for class_name in self.get_classes()])\n",
    "        micro_precision = 0.0\n",
    "        micro_recall = 0.0\n",
    "        if all_tps + all_fps > 0:\n",
    "            micro_precision = round(all_tps / (all_tps + all_fps), 4)\n",
    "        if all_tps + all_fns > 0:\n",
    "            micro_recall = round(all_tps / (all_tps + all_fns), 4)\n",
    "        if micro_precision + micro_recall > 0:\n",
    "            return round(2 * (micro_precision * micro_recall)\n",
    "                         / (micro_precision + micro_recall), 4)\n",
    "        return 0.0\n",
    "\n",
    "    def macro_avg_f_score(self):\n",
    "        class_precisions = [self.precision(class_name) for class_name in self.get_classes()]\n",
    "        class_recalls = [self.precision(class_name) for class_name in self.get_classes()]\n",
    "        macro_precision = sum(class_precisions) / len(class_precisions)\n",
    "        macro_recall = sum(class_recalls) / len(class_recalls)\n",
    "        if macro_precision + macro_recall > 0:\n",
    "            return round(2 * (macro_precision * macro_recall)\n",
    "                         / (macro_precision + macro_recall), 4)\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "\n",
    "    def accuracy(self, class_name=None):\n",
    "        if self._tps[class_name] + self._tns[class_name] + self._fps[class_name] + self._fns[class_name] > 0:\n",
    "            return round(\n",
    "                (self._tps[class_name] + self._tns[class_name])\n",
    "                / (self._tps[class_name] + self._tns[class_name] + self._fps[class_name] + self._fns[class_name]),\n",
    "                4)\n",
    "        return 0.0\n",
    "\n",
    "    def to_tsv(self):\n",
    "        return '{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}'.format(\n",
    "            self.get_tp(), self.get_tn(), self.get_fp(), self.get_fn(), self.precision(), self.recall(), self.f_score(),\n",
    "            self.accuracy())\n",
    "\n",
    "    def print(self):\n",
    "        log.info(self)\n",
    "\n",
    "    @staticmethod\n",
    "    def tsv_header(prefix=None):\n",
    "        if prefix:\n",
    "            return '{0}_TP\\t{0}_TN\\t{0}_FP\\t{0}_FN\\t{0}_PRECISION\\t{0}_RECALL\\t{0}_F-SCORE\\t{0}_ACCURACY'.format(\n",
    "                prefix)\n",
    "\n",
    "        return 'TP\\tTN\\tFP\\tFN\\tPRECISION\\tRECALL\\tF-SCORE\\tACCURACY'\n",
    "\n",
    "    @staticmethod\n",
    "    def to_empty_tsv():\n",
    "        return '_\\t_\\t_\\t_\\t_\\t_\\t_\\t_'\n",
    "\n",
    "    def __str__(self):\n",
    "        all_classes = self.get_classes()\n",
    "        all_classes = [None] + all_classes\n",
    "        all_lines = [\n",
    "            '{0:<10}\\ttp: {1} - fp: {2} - fn: {3} - tn: {4} - precision: {5:.4f} - recall: {6:.4f} - accuracy: {7:.4f} - f1-score: {8:.4f}'.format(\n",
    "                self.name if class_name == None else class_name,\n",
    "                self._tps[class_name], self._fps[class_name], self._fns[class_name], self._tns[class_name],\n",
    "                self.precision(class_name), self.recall(class_name), self.accuracy(class_name),\n",
    "                self.f_score(class_name))\n",
    "            for class_name in all_classes]\n",
    "        return '\\n'.join(all_lines)\n",
    "\n",
    "    def get_classes(self) -> List:\n",
    "        all_classes = set(itertools.chain(*[list(keys) for keys\n",
    "                                                 in [self._tps.keys(), self._fps.keys(), self._tns.keys(),\n",
    "                                                     self._fns.keys()]]))\n",
    "        all_classes = [class_name for class_name in all_classes if class_name is not None]\n",
    "        all_classes.sort()\n",
    "        return all_classes\n",
    "\n",
    "\n",
    "class WeightExtractor(object):\n",
    "\n",
    "    def __init__(self, directory: str, number_of_weights: int = 10):\n",
    "        self.weights_file = init_output_file(directory, 'weights.txt')\n",
    "        self.weights_dict = defaultdict(lambda: defaultdict(lambda: list()))\n",
    "        self.number_of_weights = number_of_weights\n",
    "\n",
    "    def extract_weights(self, state_dict, iteration):\n",
    "        for key in state_dict.keys():\n",
    "\n",
    "            vec = state_dict[key]\n",
    "            weights_to_watch = min(self.number_of_weights, reduce(lambda x, y: x * y, list(vec.size())))\n",
    "\n",
    "            if key not in self.weights_dict:\n",
    "                self._init_weights_index(key, state_dict, weights_to_watch)\n",
    "\n",
    "            for i in range(weights_to_watch):\n",
    "                vec = state_dict[key]\n",
    "                for index in self.weights_dict[key][i]:\n",
    "                    vec = vec[index]\n",
    "\n",
    "                value = vec.item()\n",
    "\n",
    "                with open(self.weights_file, 'a') as f:\n",
    "                    f.write('{}\\t{}\\t{}\\t{}\\n'.format(iteration, key, i, float(value)))\n",
    "\n",
    "    def _init_weights_index(self, key, state_dict, weights_to_watch):\n",
    "        indices = {}\n",
    "\n",
    "        i = 0\n",
    "        while len(indices) < weights_to_watch:\n",
    "            vec = state_dict[key]\n",
    "            cur_indices = []\n",
    "\n",
    "            for x in range(len(vec.size())):\n",
    "                index = random.randint(0, len(vec) - 1)\n",
    "                vec = vec[index]\n",
    "                cur_indices.append(index)\n",
    "\n",
    "            if cur_indices not in list(indices.values()):\n",
    "                indices[i] = cur_indices\n",
    "                i += 1\n",
    "\n",
    "        self.weights_dict[key] = indices\n",
    "\n",
    "\n",
    "def clear_embeddings(sentences: List[Sentence], also_clear_word_embeddings=False):\n",
    "    \"\"\"\n",
    "    Clears the embeddings from all given sentences.\n",
    "    :param sentences: list of sentences\n",
    "    \"\"\"\n",
    "    flag = also_clear_word_embeddings\n",
    "#     for sentence in sentences:\n",
    "#         sentence.clear_embeddings(also_clear_word_embeddings=False)\n",
    "\n",
    "\n",
    "def init_output_file(base_path: str, file_name: str):\n",
    "    \"\"\"\n",
    "    Creates a local file.\n",
    "    :param base_path: the path to the directory\n",
    "    :param file_name: the file name\n",
    "    :return: the created file\n",
    "    \"\"\"\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "\n",
    "    file = os.path.join(base_path, file_name)\n",
    "    open(file, \"w\", encoding='utf-8').close()\n",
    "    return file\n",
    "\n",
    "\n",
    "def convert_labels_to_one_hot(label_list: List[List[str]], label_dict: Dictionary) -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Convert list of labels (strings) to a one hot list.\n",
    "    :param label_list: list of labels\n",
    "    :param label_dict: label dictionary\n",
    "    :return: converted label list\n",
    "    \"\"\"\n",
    "    return [[1 if l in labels else 0 for l in label_dict.get_items()] for labels in label_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from abc import abstractmethod\n",
    "from typing import List, Union, Dict\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import gensim\n",
    "import numpy as np\n",
    "import torch\n",
    "from deprecated import deprecated\n",
    "from bpemb import BPEmb\n",
    "#from .nn import LockedDropout, WordDropout\n",
    "#from .data import Dictionary, Token, Sentence\n",
    "#from .file_utils import cached_path\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertModel, PRETRAINED_MODEL_ARCHIVE_MAP\n",
    "#import flair\n",
    "#from .nn import LockedDropout, WordDropout\n",
    "#from .data import Dictionary, Token, Sentence\n",
    "\n",
    "#from .file_utils import cached_path\n",
    "log = logging.getLogger('flair')\n",
    "import json\n",
    "# dean add\n",
    "CACHE_ROOT='https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/cache'\n",
    "\n",
    "class Embeddings(torch.nn.Module):\n",
    "    \"\"\"Abstract base class for all embeddings. Every new type of embedding must implement these methods.\"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def embedding_length(self) -> int:\n",
    "        \"\"\"Returns the length of the embedding vector.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def embedding_type(self) -> str:\n",
    "        pass\n",
    "\n",
    "    def embed(self, sentences: Union[Sentence, List[Sentence]]) -> List[Sentence]:\n",
    "        \"\"\"Add embeddings to all words in a list of sentences. If embeddings are already added, updates only if embeddings\n",
    "        are non-static.\"\"\"\n",
    "\n",
    "        # if only one sentence is passed, convert to list of sentence\n",
    "        if type(sentences) is Sentence:\n",
    "            sentences = [sentences]\n",
    "\n",
    "        everything_embedded: bool = True\n",
    "\n",
    "        if self.embedding_type == 'word-level':\n",
    "            for sentence in sentences:\n",
    "                for token in sentence.tokens:\n",
    "                    if self.name not in token._embeddings.keys(): everything_embedded = False\n",
    "        else:\n",
    "            for sentence in sentences:\n",
    "                if self.name not in sentence._embeddings.keys(): everything_embedded = False\n",
    "\n",
    "        if not everything_embedded or not self.static_embeddings:\n",
    "            self._add_embeddings_internal(sentences)\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    @abstractmethod\n",
    "    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n",
    "        \"\"\"Private method for adding embeddings to all words in a list of sentences.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class TokenEmbeddings(Embeddings):\n",
    "    \"\"\"Abstract base class for all token-level embeddings. Ever new type of word embedding must implement these methods.\"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def embedding_length(self) -> int:\n",
    "        \"\"\"Returns the length of the embedding vector.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def embedding_type(self) -> str:\n",
    "        return 'word-level'\n",
    "\n",
    "\n",
    "class DocumentEmbeddings(Embeddings):\n",
    "    \"\"\"Abstract base class for all document-level embeddings. Ever new type of document embedding must implement these methods.\"\"\"\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def embedding_length(self) -> int:\n",
    "        \"\"\"Returns the length of the embedding vector.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def embedding_type(self) -> str:\n",
    "        return 'sentence-level'\n",
    "\n",
    "\n",
    "class StackedEmbeddings(TokenEmbeddings):\n",
    "    \"\"\"A stack of embeddings, used if you need to combine several different embedding types.\"\"\"\n",
    "\n",
    "    def __init__(self, embeddings: List[TokenEmbeddings], detach: bool = True):\n",
    "        \"\"\"The constructor takes a list of embeddings to be combined.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "        # IMPORTANT: add embeddings as torch modules\n",
    "        for i, embedding in enumerate(embeddings):\n",
    "            self.add_module('list_embedding_{}'.format(i), embedding)\n",
    "\n",
    "        self.detach: bool = detach\n",
    "        self.name: str = 'Stack'\n",
    "        self.static_embeddings: bool = True\n",
    "\n",
    "        self.__embedding_type: str = embeddings[0].embedding_type\n",
    "\n",
    "        self.__embedding_length: int = 0\n",
    "        for embedding in embeddings:\n",
    "            self.__embedding_length += embedding.embedding_length\n",
    "\n",
    "    def embed(self, sentences: Union[Sentence, List[Sentence]], static_embeddings: bool = True):\n",
    "        # if only one sentence is passed, convert to list of sentence\n",
    "        if type(sentences) is Sentence:\n",
    "            sentences = [sentences]\n",
    "\n",
    "        for embedding in self.embeddings:\n",
    "            embedding.embed(sentences)\n",
    "\n",
    "    @property\n",
    "    def embedding_type(self) -> str:\n",
    "        return self.__embedding_type\n",
    "\n",
    "    @property\n",
    "    def embedding_length(self) -> int:\n",
    "        return self.__embedding_length\n",
    "\n",
    "    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n",
    "\n",
    "        for embedding in self.embeddings:\n",
    "            embedding._add_embeddings_internal(sentences)\n",
    "\n",
    "        return sentences\n",
    "\n",
    "\n",
    "class WordEmbeddings(TokenEmbeddings):\n",
    "    \"\"\"Standard static word embeddings, such as GloVe or FastText.\"\"\"\n",
    "\n",
    "    def __init__(self, embeddings: str):\n",
    "        \"\"\"\n",
    "        Initializes classic word embeddings. Constructor downloads required files if not there.\n",
    "        :param embeddings: one of: 'glove', 'extvec', 'crawl' or two-letter language code.\n",
    "        If you want to use a custom embedding file, just pass the path to the embeddings as embeddings variable.\n",
    "        \"\"\"\n",
    "\n",
    "        old_base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/'\n",
    "        base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.3/'\n",
    "\n",
    "        # GLOVE embeddings\n",
    "        if embeddings.lower() == 'glove' or embeddings.lower() == 'en-glove':\n",
    "            cached_path(os.path.join(old_base_path, 'glove.gensim.vectors.npy'), cache_dir='embeddings')\n",
    "            embeddings = cached_path(os.path.join(old_base_path, 'glove.gensim'), cache_dir='embeddings')\n",
    "\n",
    "        # KOMNIOS embeddings\n",
    "        elif embeddings.lower() == 'extvec' or embeddings.lower() == 'en-extvec':\n",
    "            cached_path(os.path.join(old_base_path, 'extvec.gensim.vectors.npy'), cache_dir='embeddings')\n",
    "            embeddings = cached_path(os.path.join(old_base_path, 'extvec.gensim'), cache_dir='embeddings')\n",
    "\n",
    "        # FT-CRAWL embeddings\n",
    "        elif embeddings.lower() == 'crawl' or embeddings.lower() == 'en-crawl':\n",
    "            cached_path(os.path.join(base_path, 'en-fasttext-crawl-300d-1M.vectors.npy'), cache_dir='embeddings')\n",
    "            embeddings = cached_path(os.path.join(base_path, 'en-fasttext-crawl-300d-1M'), cache_dir='embeddings')\n",
    "\n",
    "        # FT-CRAWL embeddings\n",
    "        elif embeddings.lower() == 'news' or embeddings.lower() == 'en-news' or embeddings.lower() == 'en':\n",
    "            cached_path(os.path.join(base_path, 'en-fasttext-news-300d-1M.vectors.npy'), cache_dir='embeddings')\n",
    "            embeddings = cached_path(os.path.join(base_path, 'en-fasttext-news-300d-1M'), cache_dir='embeddings')\n",
    "\n",
    "        # other language fasttext embeddings\n",
    "        elif len(embeddings.lower()) == 2 and not embeddings.lower() == 'en':\n",
    "            cached_path(os.path.join(base_path, '{}-fasttext-300d-1M.vectors.npy'.format(embeddings)),\n",
    "                        cache_dir='embeddings')\n",
    "            embeddings = cached_path(os.path.join(base_path, '{}-fasttext-300d-1M'.format(embeddings)),\n",
    "                                     cache_dir='embeddings')\n",
    "\n",
    "        elif not os.path.exists(embeddings):\n",
    "            raise ValueError(f'The given embeddings \"{embeddings}\" is not available or is not a valid path.')\n",
    "\n",
    "        self.name = embeddings\n",
    "        self.static_embeddings = True\n",
    "        \n",
    "        if embeddings.lower() in ['mimic3-i2b22010-vector-50', 'mimic3-i2b22014-vector-50'] or embeddings.lower() == 'flair/elmo_embeddings/mimic_elmo_embedding' or embeddings == \"/lrlhps/users/c272987/embeddings/PubMed-w2v.txt\":\n",
    "            self.precomputed_word_embeddings = gensim.models.KeyedVectors.load_word2vec_format(embeddings)\n",
    "        else:\n",
    "            self.precomputed_word_embeddings = gensim.models.KeyedVectors.load(embeddings)\n",
    "\n",
    "        self.__embedding_length: int = self.precomputed_word_embeddings.vector_size\n",
    "        super().__init__()\n",
    "\n",
    "    @property\n",
    "    def embedding_length(self) -> int:\n",
    "        return self.__embedding_length\n",
    "\n",
    "    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n",
    "\n",
    "        for i, sentence in enumerate(sentences):\n",
    "\n",
    "            for token, token_idx in zip(sentence.tokens, range(len(sentence.tokens))):\n",
    "                token: Token = token\n",
    "\n",
    "                if token.text in self.precomputed_word_embeddings:\n",
    "                    word_embedding = self.precomputed_word_embeddings[token.text]\n",
    "                elif token.text.lower() in self.precomputed_word_embeddings:\n",
    "                    word_embedding = self.precomputed_word_embeddings[token.text.lower()]\n",
    "                elif re.sub(r'\\d', '#', token.text.lower()) in self.precomputed_word_embeddings:\n",
    "                    word_embedding = self.precomputed_word_embeddings[re.sub(r'\\d', '#', token.text.lower())]\n",
    "                elif re.sub(r'\\d', '0', token.text.lower()) in self.precomputed_word_embeddings:\n",
    "                    word_embedding = self.precomputed_word_embeddings[re.sub(r'\\d', '0', token.text.lower())]\n",
    "                else:\n",
    "                    word_embedding = np.zeros(self.embedding_length, dtype='float')\n",
    "\n",
    "                word_embedding = torch.FloatTensor(word_embedding)\n",
    "\n",
    "                token.set_embedding(self.name, word_embedding)\n",
    "\n",
    "        return sentences\n",
    "\n",
    "class BioBertEmbeddings(TokenEmbeddings):\n",
    "    def __init__(self, biobert_file: str):\n",
    "        self.sentenceEmbedding = self.readBiobertEmbedding(biobert_file)\n",
    "        self.name = \"biobert\"\n",
    "        self.static_embeddings = True\n",
    "        self.__embedding_length:int = 768 \n",
    "        super().__init__()\n",
    "    @property\n",
    "    def embedding_length(self) -> int:\n",
    "        return self.__embedding_length\n",
    "    \n",
    "    def readBiobertEmbedding(self, biobert_file):\n",
    "        dict_fea = {}\n",
    "        with open(biobert_file) as f:\n",
    "             cur_sent = \"\"\n",
    "             cur_feature = []\n",
    "             for line in f:\n",
    "                 data = json.loads(line)\n",
    "                 #{\"linex_index\": 0, \"features\": [{\"token\": \"[CLS]\", \"layers\": [{\"index\": -1, \"values\": [0.599137,\n",
    "                 for f in data['features']:\n",
    "                     cur_token =  (f[\"token\"])\n",
    "                     if cur_token not in ['[CLS]', '[SEP]']:\n",
    "                         cur_sent += cur_token\n",
    "                         for layers in f[\"layers\"]:\n",
    "                             #print (layers.keys())\n",
    "                             if layers[\"index\"] == -1:\n",
    "                                 #print (len(layers[\"values\"]))\n",
    "                                 cur_feature.append(layers[\"values\"])\n",
    "                     if cur_token == '[SEP]':\n",
    "                         dict_fea[cur_sent.strip()] = cur_feature\n",
    "                         cur_sent = \"\"\n",
    "                         cur_feature = []\n",
    "        return dict_fea\n",
    "                              \n",
    "            \n",
    "    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n",
    "\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sent_txt = sentence.to_plain_string().replace(\" \", \"\")\n",
    "            sent_embedding = []\n",
    "            if sent_txt in self.sentenceEmbedding:\n",
    "                sent_embedding = self.sentenceEmbedding[sent_txt]\n",
    "            for token, token_idx in zip(sentence.tokens, range(len(sentence.tokens))):\n",
    "                token: Token = token\n",
    "                if len( sent_embedding) == len(sentence.tokens):\n",
    "                    word_embedding = sent_embedding[token_idx]\n",
    "                    word_embedding = torch.FloatTensor(word_embedding)\n",
    "                    token.set_embedding(self.name, word_embedding)\n",
    "                else:\n",
    "                    word_embedding = [1] * 768\n",
    "                    word_embedding = torch.FloatTensor(word_embedding)\n",
    "                    token.set_embedding(self.name, word_embedding)\n",
    "        return sentences\n",
    "\n",
    "class SemanticEmbeddings(TokenEmbeddings):\n",
    "    \"\"\"Standard static word embeddings, such as GloVe or FastText.\"\"\"\n",
    "\n",
    "    def __init__(self, embeddings: str, semantic_file: str):\n",
    "        \"\"\"\n",
    "        Initializes classic word embeddings. Constructor downloads required files if not there.\n",
    "        :param embeddings: one of: 'glove', 'extvec', 'crawl' or two-letter language code.\n",
    "        If you want to use a custom embedding file, just pass the path to the embeddings as embeddings variable.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        self.name = embeddings\n",
    "        self.static_embeddings = True\n",
    "        self.semantic_file = semantic_file\n",
    "        if embeddings.lower() in ['mimic3-i2b22014-vector-dic-50', 'mimic3-i2b22010-vector-dic-50', 'flair/elmo_embeddings/mimic_semantic_elmo_embedding']:\n",
    "            self.precomputed_word_embeddings = gensim.models.KeyedVectors.load_word2vec_format(embeddings)\n",
    "        else:\n",
    "            self.precomputed_word_embeddings = gensim.models.KeyedVectors.load(embeddings)\n",
    "        self.semantic_dict = {}\n",
    "        self.initSemanticDict()\n",
    "\n",
    "        self.__embedding_length: int = self.precomputed_word_embeddings.vector_size\n",
    "        super().__init__()\n",
    "\n",
    "    def initSemanticDict(self):\n",
    "        words = []\n",
    "        sems = []\n",
    "        for line in open(self.semantic_file).readlines():\n",
    "            #print (line.strip())\n",
    "            if line.strip() == \"\":\n",
    "                key = \" \".join(words)\n",
    "                self.semantic_dict[key] = sems\n",
    "                #print (key, sems)\n",
    "                words = []\n",
    "                sems = []\n",
    "            else:    \n",
    "                items = line.strip().split(\"\\t\")\n",
    "                words.append(items[0])\n",
    "                sems.append(items[1])\n",
    "                \n",
    "        key = \" \".join(words)\n",
    "        self.semantic_dict[key] = sems\n",
    "        words = []\n",
    "        sems = []   \n",
    "\n",
    "    def getSemanticSentence(self, tokens: List[Token]):\n",
    "        #print (self.semantic_dict.items()[:10])\n",
    "        #print(\"calling getSemanticSentence\")\n",
    "        keys = [token.text for token in tokens]\n",
    "        key = \" \".join(keys)\n",
    "        #print (key)\n",
    "        sems = []\n",
    "        if key in self.semantic_dict:\n",
    "            sems = self.semantic_dict[key]\n",
    "        else:\n",
    "            print (key + \" was not found\")\n",
    "        for i in range(len(sems)):\n",
    "            tokens[i].set_sem(sems[i])\n",
    "            #print (tokens[i].sem)\n",
    "        return tokens\n",
    "\n",
    "    @property\n",
    "    def embedding_length(self) -> int:\n",
    "        return self.__embedding_length\n",
    "\n",
    "    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n",
    "\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sem_tokens = self.getSemanticSentence(sentence.tokens)\n",
    "            #for token in sem_tokens:\n",
    "            #    print (token.text, token.sem)\n",
    "            for token, token_idx in zip(sem_tokens, range(len(sem_tokens))):\n",
    "                token: Token = token\n",
    "\n",
    "                if token.text in self.precomputed_word_embeddings:\n",
    "                    word_embedding = self.precomputed_word_embeddings[token.sem]\n",
    "                elif token.sem.lower() in self.precomputed_word_embeddings:\n",
    "                    word_embedding = self.precomputed_word_embeddings[token.sem.lower()]\n",
    "                elif re.sub(r'\\d', '#', token.sem.lower()) in self.precomputed_word_embeddings:\n",
    "                    word_embedding = self.precomputed_word_embeddings[re.sub(r'\\d', '#', token.sem.lower())]\n",
    "                elif re.sub(r'\\d', '0', token.sem.lower()) in self.precomputed_word_embeddings:\n",
    "                    word_embedding = self.precomputed_word_embeddings[re.sub(r'\\d', '0', token.sem.lower())]\n",
    "                else:\n",
    "                    word_embedding = np.zeros(self.embedding_length, dtype='float')\n",
    "\n",
    "                word_embedding = torch.FloatTensor(word_embedding)\n",
    "\n",
    "                token.set_embedding(self.name, word_embedding)\n",
    "\n",
    "        return sentences\n",
    "\n",
    "class MemoryEmbeddings(TokenEmbeddings):\n",
    "\n",
    "    def __init__(self, tag_type: str, tag_dictionary: Dictionary):\n",
    "\n",
    "        self.name = \"memory\"\n",
    "        self.static_embeddings = False\n",
    "        self.tag_type: str = tag_type\n",
    "        self.tag_dictionary: Dictionary = tag_dictionary\n",
    "        self.__embedding_length: int = len(tag_dictionary)\n",
    "\n",
    "        self.memory: Dict[str:List] = {}\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "    @property\n",
    "    def embedding_length(self) -> int:\n",
    "        return self.__embedding_length\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        super().train(mode=mode)\n",
    "        if mode:\n",
    "            self.memory: Dict[str:List] = {}\n",
    "\n",
    "    def update_embedding(self, text: str, tag: str):\n",
    "        self.memory[text][self.tag_dictionary.get_idx_for_item(tag)] += 1\n",
    "\n",
    "    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n",
    "\n",
    "        for i, sentence in enumerate(sentences):\n",
    "\n",
    "            for token, token_idx in zip(sentence.tokens, range(len(sentence.tokens))):\n",
    "                token: Token = token\n",
    "\n",
    "                if token.text not in self.memory:\n",
    "                    self.memory[token.text] = [0] * self.__embedding_length\n",
    "\n",
    "                word_embedding = torch.FloatTensor(self.memory[token.text])\n",
    "                import torch.nn.functional as F\n",
    "                word_embedding = F.normalize(word_embedding, p=2, dim=0)\n",
    "\n",
    "                token.set_embedding(self.name, word_embedding)\n",
    "\n",
    "                # add label if in training mode\n",
    "                if self.training:\n",
    "                    self.update_embedding(token.text, token.get_tag(self.tag_type).value)\n",
    "\n",
    "        return sentences\n",
    "\n",
    "\n",
    "class BPEmbSerializable(BPEmb):\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = self.__dict__.copy()\n",
    "        # save the sentence piece model as binary file (not as path which may change)\n",
    "        state['spm_model_binary'] = open(self.model_file, mode='rb').read()\n",
    "        state['spm'] = None\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        from bpemb.util import sentencepiece_load\n",
    "        model_file = self.model_tpl.format(lang=state['lang'], vs=state['vs'])\n",
    "        self.__dict__ = state\n",
    "\n",
    "        # write out the binary sentence piece model into the expected directory\n",
    "#         self.cache_dir: Path = Path(flair.file_utils.CACHE_ROOT) / 'embeddings'\n",
    "        # dean add\n",
    "        self.cache_dir: Path = CACHE_ROOT +'/embeddings'\n",
    "        if 'spm_model_binary' in self.__dict__:\n",
    "            # if the model was saved as binary and it is not found on disk, write to appropriate path\n",
    "            if not os.path.exists(self.cache_dir / state['lang']):\n",
    "                os.makedirs(self.cache_dir / state['lang'])\n",
    "            self.model_file = self.cache_dir / model_file\n",
    "            with open(self.model_file, 'wb') as out:\n",
    "                out.write(self.__dict__['spm_model_binary'])\n",
    "        else:\n",
    "            # otherwise, use normal process and potentially trigger another download\n",
    "            self.model_file = self._load_file(model_file)\n",
    "\n",
    "        # once the modes if there, load it with sentence piece\n",
    "        state['spm'] = sentencepiece_load(self.model_file)\n",
    "\n",
    "class BytePairEmbeddings(TokenEmbeddings):\n",
    "\n",
    "    def __init__(self, language: str, dim: int = 50, syllables: int = 100000, cache_dir = CACHE_ROOT+ '/embeddings'):\n",
    "#         def __init__(self, language: str, dim: int = 50, syllables: int = 100000, cache_dir = Path(flair.file_utils.CACHE_ROOT) / 'embeddings'):\n",
    "        \"\"\"\n",
    "        Initializes BP embeddings. Constructor downloads required files if not there.\n",
    "        \"\"\"\n",
    "\n",
    "        self.name: str = f'bpe-{language}-{syllables}-{dim}'\n",
    "        self.static_embeddings = True\n",
    "        self.embedder = BPEmbSerializable(lang=language, vs=syllables, dim=dim, cache_dir=cache_dir)\n",
    "\n",
    "        self.__embedding_length: int = self.embedder.emb.vector_size * 2\n",
    "        super().__init__()\n",
    "\n",
    "    @property\n",
    "    def embedding_length(self) -> int:\n",
    "        return self.__embedding_length\n",
    "\n",
    "    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n",
    "\n",
    "        for i, sentence in enumerate(sentences):\n",
    "\n",
    "            for token, token_idx in zip(sentence.tokens, range(len(sentence.tokens))):\n",
    "                token: Token = token\n",
    "\n",
    "                if 'field' not in self.__dict__ or self.field is None:\n",
    "                    word = token.text\n",
    "                else:\n",
    "                    word = token.get_tag(self.field).value\n",
    "\n",
    "                embeddings = self.embedder.embed(word.lower())\n",
    "                embedding = np.concatenate((embeddings[0], embeddings[len(embeddings)-1]))\n",
    "                token.set_embedding(self.name, torch.tensor(embedding, dtype=torch.float))\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "\n",
    "\n",
    "class ELMoEmbeddings(TokenEmbeddings):\n",
    "    \"\"\"Contextual word embeddings using word-level LM, as proposed in Peters et al., 2018.\"\"\"\n",
    "\n",
    "    def __init__(self, model: str = 'original'):\n",
    "        super().__init__()\n",
    "\n",
    "        try:\n",
    "            import allennlp.commands.elmo\n",
    "        except:\n",
    "            log.warning('-' * 100)\n",
    "            log.warning('ATTENTION! The library \"allennlp\" is not installed!')\n",
    "            log.warning('To use ELMoEmbeddings, please first install with \"pip install allennlp\"')\n",
    "            log.warning('-' * 100)\n",
    "            pass\n",
    "\n",
    "        self.name = 'elmo-' + model\n",
    "        self.static_embeddings = True\n",
    "\n",
    "        # the default model for ELMo is the 'original' model, which is very large\n",
    "        options_file = allennlp.commands.elmo.DEFAULT_OPTIONS_FILE\n",
    "        weight_file = allennlp.commands.elmo.DEFAULT_WEIGHT_FILE\n",
    "        # alternatively, a small, medium or portuguese model can be selected by passing the appropriate mode name\n",
    "        if model == 'small':\n",
    "            options_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_options.json'\n",
    "            weight_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_weights.hdf5'\n",
    "        if model == 'medium':\n",
    "            options_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x2048_256_2048cnn_1xhighway/elmo_2x2048_256_2048cnn_1xhighway_options.json'\n",
    "            weight_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x2048_256_2048cnn_1xhighway/elmo_2x2048_256_2048cnn_1xhighway_weights.hdf5'\n",
    "        if model == 'pt' or model == 'portuguese':\n",
    "            options_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/contributed/pt/elmo_pt_options.json'\n",
    "            weight_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/contributed/pt/elmo_pt_weights.hdf5'\n",
    "        if model == 'pubmed':\n",
    "            options_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/contributed/pubmed/elmo_2x4096_512_2048cnn_2xhighway_options.json'\n",
    "            weight_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/contributed/pubmed/elmo_2x4096_512_2048cnn_2xhighway_weights_PubMed_only.hdf5'\n",
    "\n",
    "        # put on Cuda if available\n",
    "#         from flair import device\n",
    "#         import de\n",
    "        cuda_device = 0 if str(device) != 'cpu' else -1\n",
    "        self.ee = allennlp.commands.elmo.ElmoEmbedder(options_file=options_file,\n",
    "                                                      weight_file=weight_file,\n",
    "                                                      cuda_device=cuda_device)\n",
    "\n",
    "        # embed a dummy sentence to determine embedding_length\n",
    "        dummy_sentence: Sentence = Sentence()\n",
    "        dummy_sentence.add_token(Token('hello'))\n",
    "        embedded_dummy = self.embed(dummy_sentence)\n",
    "        self.__embedding_length: int = len(embedded_dummy[0].get_token(1).get_embedding())\n",
    "\n",
    "    @property\n",
    "    def embedding_length(self) -> int:\n",
    "        return self.__embedding_length\n",
    "\n",
    "    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n",
    "\n",
    "        sentence_words: List[List[str]] = []\n",
    "        for sentence in sentences:\n",
    "            sentence_words.append([token.text for token in sentence])\n",
    "\n",
    "        embeddings = self.ee.embed_batch(sentence_words)\n",
    "\n",
    "        for i, sentence in enumerate(sentences):\n",
    "\n",
    "            sentence_embeddings = embeddings[i]\n",
    "\n",
    "            for token, token_idx in zip(sentence.tokens, range(len(sentence.tokens))):\n",
    "                token: Token = token\n",
    "\n",
    "                word_embedding = torch.cat([\n",
    "                    torch.FloatTensor(sentence_embeddings[0, token_idx, :]),\n",
    "                    torch.FloatTensor(sentence_embeddings[1, token_idx, :]),\n",
    "                    torch.FloatTensor(sentence_embeddings[2, token_idx, :])\n",
    "                ], 0)\n",
    "\n",
    "                token.set_embedding(self.name, word_embedding)\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'model={}'.format(self.name)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "\n",
    "class ELMoTransformerEmbeddings(TokenEmbeddings):\n",
    "    \"\"\"Contextual word embeddings using word-level Transformer-based LM, as proposed in Peters et al., 2018.\"\"\"\n",
    "\n",
    "    def __init__(self, model_file: str):\n",
    "        super().__init__()\n",
    "\n",
    "        try:\n",
    "            from allennlp.modules.token_embedders.bidirectional_language_model_token_embedder import \\\n",
    "                BidirectionalLanguageModelTokenEmbedder\n",
    "            from allennlp.data.token_indexers.elmo_indexer import ELMoTokenCharactersIndexer\n",
    "        except:\n",
    "            log.warning('-' * 100)\n",
    "            log.warning('ATTENTION! The library \"allennlp\" is not installed!')\n",
    "            log.warning(\n",
    "                'To use ELMoTransformerEmbeddings, please first install a recent version from https://github.com/allenai/allennlp')\n",
    "            log.warning('-' * 100)\n",
    "            pass\n",
    "\n",
    "        self.name = 'elmo-transformer'\n",
    "        self.static_embeddings = True\n",
    "        self.lm_embedder = BidirectionalLanguageModelTokenEmbedder(\n",
    "            archive_file=model_file,\n",
    "            dropout=0.2,\n",
    "            bos_eos_tokens=(\"<S>\", \"</S>\"),\n",
    "            remove_bos_eos=True,\n",
    "            requires_grad=False\n",
    "        )\n",
    "        self.lm_embedder = self.lm_embedder.to(device=flair.device)\n",
    "        self.vocab = self.lm_embedder._lm.vocab\n",
    "        self.indexer = ELMoTokenCharactersIndexer()\n",
    "\n",
    "        # embed a dummy sentence to determine embedding_length\n",
    "        dummy_sentence: Sentence = Sentence()\n",
    "        dummy_sentence.add_token(Token('hello'))\n",
    "        embedded_dummy = self.embed(dummy_sentence)\n",
    "        self.__embedding_length: int = len(embedded_dummy[0].get_token(1).get_embedding())\n",
    "\n",
    "    @property\n",
    "    def embedding_length(self) -> int:\n",
    "        return self.__embedding_length\n",
    "\n",
    "    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n",
    "        # Avoid conflicts with flair's Token class\n",
    "        import allennlp.data.tokenizers.token as allen_nlp_token\n",
    "\n",
    "        indexer = self.indexer\n",
    "        vocab = self.vocab\n",
    "\n",
    "        for sentence in sentences:\n",
    "            character_indices = indexer.tokens_to_indices([allen_nlp_token.Token(token.text) for token in sentence],\n",
    "                                                          vocab, \"elmo\")[\"elmo\"]\n",
    "\n",
    "            indices_tensor = torch.LongTensor([character_indices])\n",
    "            indices_tensor = indices_tensor.to(device=flair.device)\n",
    "            embeddings = self.lm_embedder(indices_tensor)[0].detach().cpu().numpy()\n",
    "\n",
    "            for token, token_idx in zip(sentence.tokens, range(len(sentence.tokens))):\n",
    "                token: Token = token\n",
    "                embedding = embeddings[token_idx]\n",
    "                word_embedding = torch.FloatTensor(embedding)\n",
    "                token.set_embedding(self.name, word_embedding)\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'model={}'.format(self.name)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "class CharacterEmbeddings(TokenEmbeddings):\n",
    "    \"\"\"Character embeddings of words, as proposed in Lample et al., 2016.\"\"\"\n",
    "\n",
    "    def __init__(self, path_to_char_dict: str = None):\n",
    "        \"\"\"Uses the default character dictionary if none provided.\"\"\"\n",
    "\n",
    "        super(CharacterEmbeddings, self).__init__()\n",
    "        self.name = 'Char'\n",
    "        self.static_embeddings = False\n",
    "\n",
    "        # use list of common characters if none provided\n",
    "        if path_to_char_dict is None:\n",
    "            self.char_dictionary: Dictionary = Dictionary.load('common-chars')\n",
    "        else:\n",
    "            self.char_dictionary: Dictionary = Dictionary.load_from_file(path_to_char_dict)\n",
    "\n",
    "        self.char_embedding_dim: int = 25\n",
    "        self.hidden_size_char: int = 25\n",
    "        self.char_embedding = torch.nn.Embedding(len(self.char_dictionary.item2idx), self.char_embedding_dim)\n",
    "        self.char_rnn = torch.nn.LSTM(self.char_embedding_dim, self.hidden_size_char, num_layers=1,\n",
    "                                      bidirectional=True)\n",
    "\n",
    "        self.__embedding_length = self.char_embedding_dim * 2\n",
    "\n",
    "    @property\n",
    "    def embedding_length(self) -> int:\n",
    "        return self.__embedding_length\n",
    "\n",
    "    def _add_embeddings_internal(self, sentences: List[Sentence]):\n",
    "\n",
    "        for sentence in sentences:\n",
    "\n",
    "            tokens_char_indices = []\n",
    "\n",
    "            # translate words in sentence into ints using dictionary\n",
    "            for token in sentence.tokens:\n",
    "                token: Token = token\n",
    "                char_indices = [self.char_dictionary.get_idx_for_item(char) for char in token.text]\n",
    "                tokens_char_indices.append(char_indices)\n",
    "\n",
    "            # sort words by length, for batching and masking\n",
    "            tokens_sorted_by_length = sorted(tokens_char_indices, key=lambda p: len(p), reverse=True)\n",
    "            d = {}\n",
    "            for i, ci in enumerate(tokens_char_indices):\n",
    "                for j, cj in enumerate(tokens_sorted_by_length):\n",
    "                    if ci == cj:\n",
    "                        d[j] = i\n",
    "                        continue\n",
    "            chars2_length = [len(c) for c in tokens_sorted_by_length]\n",
    "            longest_token_in_sentence = max(chars2_length)\n",
    "            tokens_mask = np.zeros((len(tokens_sorted_by_length), longest_token_in_sentence), dtype='int')\n",
    "            for i, c in enumerate(tokens_sorted_by_length):\n",
    "                tokens_mask[i, :chars2_length[i]] = c\n",
    "\n",
    "            tokens_mask = torch.LongTensor(tokens_mask)\n",
    "\n",
    "            # chars for rnn processing\n",
    "            chars = tokens_mask\n",
    "            if torch.cuda.is_available():\n",
    "                chars = chars.cuda()\n",
    "\n",
    "            character_embeddings = self.char_embedding(chars).transpose(0, 1)\n",
    "\n",
    "            packed = torch.nn.utils.rnn.pack_padded_sequence(character_embeddings, chars2_length)\n",
    "\n",
    "            lstm_out, self.hidden = self.char_rnn(packed)\n",
    "\n",
    "            outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(lstm_out)\n",
    "            outputs = outputs.transpose(0, 1)\n",
    "            chars_embeds_temp = torch.FloatTensor(torch.zeros((outputs.size(0), outputs.size(2))))\n",
    "            if torch.cuda.is_available():\n",
    "                chars_embeds_temp = chars_embeds_temp.cuda()\n",
    "            for i, index in enumerate(output_lengths):\n",
    "                chars_embeds_temp[i] = outputs[i, index - 1]\n",
    "            character_embeddings = chars_embeds_temp.clone()\n",
    "            for i in range(character_embeddings.size(0)):\n",
    "                character_embeddings[d[i]] = chars_embeds_temp[i]\n",
    "\n",
    "            for token_number, token in enumerate(sentence.tokens):\n",
    "                token.set_embedding(self.name, character_embeddings[token_number])\n",
    "\n",
    "\n",
    "class FlairEmbeddings(TokenEmbeddings):\n",
    "    \"\"\"Contextual string embeddings of words, as proposed in Akbik et al., 2018.\"\"\"\n",
    "\n",
    "    def __init__(self, model: str, use_cache: bool = False, cache_directory: Path = None, chars_per_chunk: int = 512):\n",
    "        \"\"\"\n",
    "        initializes contextual string embeddings using a character-level language model.\n",
    "        :param model: model string, one of 'news-forward', 'news-backward', 'news-forward-fast', 'news-backward-fast',\n",
    "                'mix-forward', 'mix-backward', 'german-forward', 'german-backward', 'polish-backward', 'polish-forward'\n",
    "                depending on which character language model is desired.\n",
    "        :param use_cache: if set to False, will not write embeddings to file for later retrieval. this saves disk space but will\n",
    "                not allow re-use of once computed embeddings that do not fit into memory\n",
    "        :param cache_directory: if cache_directory is not set, the cache will be written to ~/.flair/embeddings. otherwise the cache\n",
    "                is written to the provided directory.\n",
    "        :param  chars_per_chunk: max number of chars per rnn pass to control speed/memory tradeoff. Higher means faster but requires\n",
    "                more memory. Lower means slower but less memory.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        cache_dir = Path('embeddings')\n",
    "\n",
    "        # multilingual forward (English, German, French, Italian, Dutch, Polish)\n",
    "        if model.lower() == 'multi-forward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-multi-forward-v0.1.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "        # multilingual backward  (English, German, French, Italian, Dutch, Polish)\n",
    "        elif model.lower() == 'multi-backward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-multi-backward-v0.1.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "\n",
    "        # multilingual forward fast (English, German, French, Italian, Dutch, Polish)\n",
    "        elif model.lower() == 'multi-forward-fast':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-multi-forward-fast-v0.1.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "        # multilingual backward fast (English, German, French, Italian, Dutch, Polish)\n",
    "        elif model.lower() == 'multi-backward-fast':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-multi-backward-fast-v0.1.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "\n",
    "        # news-english-forward\n",
    "        elif model.lower() == 'news-forward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-forward-v0.2rc.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "\n",
    "        # news-english-backward\n",
    "        elif model.lower() == 'news-backward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-backward-v0.2rc.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "\n",
    "        # news-english-forward\n",
    "        elif model.lower() == 'news-forward-fast':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-forward-1024-v0.2rc.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "\n",
    "        # news-english-backward\n",
    "        elif model.lower() == 'news-backward-fast':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-backward-1024-v0.2rc.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "\n",
    "        # mix-english-forward\n",
    "        elif model.lower() == 'mix-forward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-mix-english-forward-v0.2rc.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "\n",
    "        # mix-english-backward\n",
    "        elif model.lower() == 'mix-backward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-mix-english-backward-v0.2rc.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "\n",
    "        # mix-german-forward\n",
    "        elif model.lower() == 'german-forward' or model.lower() == 'de-forward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-mix-german-forward-v0.2rc.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "\n",
    "        # mix-german-backward\n",
    "        elif model.lower() == 'german-backward' or model.lower() == 'de-backward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-mix-german-backward-v0.2rc.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "\n",
    "        # common crawl Polish forward\n",
    "        elif model.lower() == 'polish-forward' or model.lower() == 'pl-forward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-polish-forward-v0.2.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "\n",
    "        # common crawl Polish backward\n",
    "        elif model.lower() == 'polish-backward' or model.lower() == 'pl-backward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-polish-backward-v0.2.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "\n",
    "        # Slovenian forward\n",
    "        elif model.lower() == 'slovenian-forward' or model.lower() == 'sl-forward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.3/lm-sl-large-forward-v0.1.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "        # Slovenian backward\n",
    "        elif model.lower() == 'slovenian-backward' or model.lower() == 'sl-backward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.3/lm-sl-large-backward-v0.1.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "\n",
    "        # Bulgarian forward\n",
    "        elif model.lower() == 'bulgarian-forward' or model.lower() == 'bg-forward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.3/lm-bg-small-forward-v0.1.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "        # Bulgarian backward\n",
    "        elif model.lower() == 'bulgarian-backward' or model.lower() == 'bg-backward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.3/lm-bg-small-backward-v0.1.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "\n",
    "        # Dutch forward\n",
    "        elif model.lower() == 'dutch-forward' or model.lower() == 'nl-forward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-nl-large-forward-v0.1.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "        # Dutch backward\n",
    "        elif model.lower() == 'dutch-backward' or model.lower() == 'nl-backward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-nl-large-backward-v0.1.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "\n",
    "        # Swedish forward\n",
    "        elif model.lower() == 'swedish-forward' or model.lower() == 'sv-forward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-sv-large-forward-v0.1.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "        # Swedish backward\n",
    "        elif model.lower() == 'swedish-backward' or model.lower() == 'sv-backward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-sv-large-backward-v0.1.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "\n",
    "        # French forward\n",
    "        elif model.lower() == 'french-forward' or model.lower() == 'fr-forward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-fr-charlm-forward.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "        # French backward\n",
    "        elif model.lower() == 'french-backward' or model.lower() == 'fr-backward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-fr-charlm-backward.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "\n",
    "        # Czech forward\n",
    "        elif model.lower() == 'czech-forward' or model.lower() == 'cs-forward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-cs-large-forward-v0.1.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "        # Czech backward\n",
    "        elif model.lower() == 'czech-backward' or model.lower() == 'cs-backward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-cs-large-backward-v0.1.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "\n",
    "        # Portuguese forward\n",
    "        elif model.lower() == 'portuguese-forward' or model.lower() == 'pt-forward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-pt-forward.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "        # Portuguese backward\n",
    "        elif model.lower() == 'portuguese-backward' or model.lower() == 'pt-backward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-pt-backward.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "\n",
    "        # Basque forward\n",
    "        elif model.lower() == 'basque-forward' or model.lower() == 'eu-forward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-eu-large-forward-v0.1.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "        # Basque backward\n",
    "        elif model.lower() == 'basque-backward' or model.lower() == 'eu-backward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/lm-eu-large-backward-v0.1.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "\n",
    "        # Spanish forward fast\n",
    "        elif model.lower() == 'spanish-forward-fast' or model.lower() == 'es-forward-fast':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/language_model_es_forward/lm-es-forward-fast.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "        # Spanish backward fast\n",
    "        elif model.lower() == 'spanish-backward-fast' or model.lower() == 'es-backward-fast':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/language_model_es_backward/lm-es-backward-fast.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "\n",
    "        # Spanish forward\n",
    "        elif model.lower() == 'spanish-forward' or model.lower() == 'es-forward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/language_model_es_forward_long/lm-es-forward.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "        # Spanish backward\n",
    "        elif model.lower() == 'spanish-backward' or model.lower() == 'es-backward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4/language_model_es_backward_long/lm-es-backward.pt'\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "        # Pubmed forward\n",
    "        elif model.lower() == \"pubmed-forward\":\n",
    "            base_path = \"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4.1/pubmed-2015-fw-lm.pt\"\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "        # Pubmed backward\n",
    "        elif model.lower() == \"pubmed-backward\":\n",
    "            base_path = \"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4.1/pubmed-2015-bw-lm.pt\"\n",
    "            model = cached_path(base_path, cache_dir=cache_dir)\n",
    "\n",
    "        elif not Path(model).exists():\n",
    "            raise ValueError(f'The given model \"{model}\" is not available or is not a valid path.')\n",
    "\n",
    "        self.name = str(model)\n",
    "        self.static_embeddings = True\n",
    "\n",
    "#         from flair.models import LanguageModel\n",
    "        self.lm = LanguageModel.load_language_model(model)\n",
    "\n",
    "        self.is_forward_lm: bool = self.lm.is_forward_lm\n",
    "        self.chars_per_chunk: int = chars_per_chunk\n",
    "\n",
    "        # initialize cache if use_cache set\n",
    "        self.cache = None\n",
    "        if use_cache:\n",
    "            cache_path = Path(f'{self.name}-tmp-cache.sqllite') if not cache_directory else \\\n",
    "                cache_directory / f'{self.name}-tmp-cache.sqllite'\n",
    "            from sqlitedict import SqliteDict\n",
    "            self.cache = SqliteDict(str(cache_path), autocommit=True)\n",
    "\n",
    "        # embed a dummy sentence to determine embedding_length\n",
    "        dummy_sentence: Sentence = Sentence()\n",
    "        dummy_sentence.add_token(Token('hello'))\n",
    "        embedded_dummy = self.embed(dummy_sentence)\n",
    "        self.__embedding_length: int = len(embedded_dummy[0].get_token(1).get_embedding())\n",
    "\n",
    "        # set to eval mode\n",
    "        self.eval()\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        pass\n",
    "\n",
    "    def __getstate__(self):\n",
    "        # Copy the object's state from self.__dict__ which contains\n",
    "        # all our instance attributes. Always use the dict.copy()\n",
    "        # method to avoid modifying the original state.\n",
    "        state = self.__dict__.copy()\n",
    "        # Remove the unpicklable entries.\n",
    "        state['cache'] = None\n",
    "        return state\n",
    "\n",
    "    @property\n",
    "    def embedding_length(self) -> int:\n",
    "        return self.__embedding_length\n",
    "\n",
    "    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n",
    "\n",
    "        # make compatible with serialized models\n",
    "        if 'chars_per_chunk' not in self.__dict__:\n",
    "            self.chars_per_chunk = 512\n",
    "\n",
    "        # if cache is used, try setting embeddings from cache first\n",
    "        if 'cache' in self.__dict__ and self.cache is not None:\n",
    "\n",
    "            # try populating embeddings from cache\n",
    "            all_embeddings_retrieved_from_cache: bool = True\n",
    "            for sentence in sentences:\n",
    "                key = sentence.to_tokenized_string()\n",
    "                embeddings = self.cache.get(key)\n",
    "\n",
    "                if not embeddings:\n",
    "                    all_embeddings_retrieved_from_cache = False\n",
    "                    break\n",
    "                else:\n",
    "                    for token, embedding in zip(sentence, embeddings):\n",
    "                        token.set_embedding(self.name, torch.FloatTensor(embedding))\n",
    "\n",
    "            if all_embeddings_retrieved_from_cache:\n",
    "                return sentences\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # if this is not possible, use LM to generate embedding. First, get text sentences\n",
    "            text_sentences = [sentence.to_tokenized_string() for sentence in sentences]\n",
    "\n",
    "            longest_character_sequence_in_batch: int = len(max(text_sentences, key=len))\n",
    "\n",
    "            # pad strings with whitespaces to longest sentence\n",
    "            sentences_padded: List[str] = []\n",
    "            append_padded_sentence = sentences_padded.append\n",
    "\n",
    "            start_marker = '\\n'\n",
    "\n",
    "            end_marker = ' '\n",
    "            extra_offset = len(start_marker)\n",
    "            for sentence_text in text_sentences:\n",
    "                pad_by = longest_character_sequence_in_batch - len(sentence_text)\n",
    "                if self.is_forward_lm:\n",
    "                    padded = '{}{}{}{}'.format(start_marker, sentence_text, end_marker, pad_by * ' ')\n",
    "                    append_padded_sentence(padded)\n",
    "                else:\n",
    "                    padded = '{}{}{}{}'.format(start_marker, sentence_text[::-1], end_marker, pad_by * ' ')\n",
    "                    append_padded_sentence(padded)\n",
    "\n",
    "            # get hidden states from language model\n",
    "            all_hidden_states_in_lm = self.lm.get_representation(sentences_padded, self.chars_per_chunk)\n",
    "\n",
    "            # take first or last hidden states from language model as word representation\n",
    "            for i, sentence in enumerate(sentences):\n",
    "                sentence_text = sentence.to_tokenized_string()\n",
    "\n",
    "                offset_forward: int = extra_offset\n",
    "                offset_backward: int = len(sentence_text) + extra_offset\n",
    "\n",
    "                for token in sentence.tokens:\n",
    "                    token: Token = token\n",
    "\n",
    "                    offset_forward += len(token.text)\n",
    "\n",
    "                    if self.is_forward_lm:\n",
    "                        offset = offset_forward\n",
    "                    else:\n",
    "                        offset = offset_backward\n",
    "\n",
    "                    embedding = all_hidden_states_in_lm[offset, i, :]\n",
    "\n",
    "                    # if self.tokenized_lm or token.whitespace_after:\n",
    "                    offset_forward += 1\n",
    "                    offset_backward -= 1\n",
    "\n",
    "                    offset_backward -= len(token.text)\n",
    "\n",
    "                    token.set_embedding(self.name, embedding.clone().detach())\n",
    "\n",
    "            all_hidden_states_in_lm = None\n",
    "\n",
    "        if 'cache' in self.__dict__ and self.cache is not None:\n",
    "            for sentence in sentences:\n",
    "                self.cache[sentence.to_tokenized_string()] = [token._embeddings[self.name].tolist() for token in\n",
    "                                                              sentence]\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "\n",
    "\n",
    "class PooledFlairEmbeddings(TokenEmbeddings):\n",
    "\n",
    "    def __init__(self,\n",
    "                 contextual_embeddings: Union[str, FlairEmbeddings],\n",
    "                 pooling: str = 'fade',\n",
    "                 only_capitalized: bool = False,\n",
    "                 **kwargs):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # use the character language model embeddings as basis\n",
    "        if type(contextual_embeddings) is str:\n",
    "            self.context_embeddings: FlairEmbeddings = FlairEmbeddings(contextual_embeddings, **kwargs)\n",
    "        else:\n",
    "            self.context_embeddings: FlairEmbeddings = contextual_embeddings\n",
    "\n",
    "        # length is twice the original character LM embedding length\n",
    "        self.embedding_length = self.context_embeddings.embedding_length * 2\n",
    "        self.name = self.context_embeddings.name + '-context'\n",
    "\n",
    "        # these fields are for the embedding memory\n",
    "        self.word_embeddings = {}\n",
    "        self.word_count = {}\n",
    "\n",
    "        # whether to add only capitalized words to memory (faster runtime and lower memory consumption)\n",
    "        self.only_capitalized = only_capitalized\n",
    "\n",
    "        # we re-compute embeddings dynamically at each epoch\n",
    "        self.static_embeddings = False\n",
    "\n",
    "        # set the memory method\n",
    "        self.pooling = pooling\n",
    "        if pooling == 'mean':\n",
    "            self.aggregate_op = torch.add\n",
    "        elif pooling == 'fade':\n",
    "            self.aggregate_op = torch.add\n",
    "        elif pooling == 'max':\n",
    "            self.aggregate_op = torch.max\n",
    "        elif pooling == 'min':\n",
    "            self.aggregate_op = torch.min\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        super().train(mode=mode)\n",
    "        if mode:\n",
    "            # memory is wiped each time we do a training run\n",
    "            print('train mode resetting embeddings')\n",
    "            self.word_embeddings = {}\n",
    "            self.word_count = {}\n",
    "\n",
    "    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n",
    "\n",
    "        self.context_embeddings.embed(sentences)\n",
    "\n",
    "        # if we keep a pooling, it needs to be updated continuously\n",
    "        for sentence in sentences:\n",
    "            for token in sentence.tokens:\n",
    "\n",
    "                # update embedding\n",
    "                local_embedding = token._embeddings[self.context_embeddings.name]\n",
    "\n",
    "                if token.text[0].isupper() or not self.only_capitalized:\n",
    "\n",
    "                    if token.text not in self.word_embeddings:\n",
    "                        self.word_embeddings[token.text] = local_embedding\n",
    "                        self.word_count[token.text] = 1\n",
    "                    else:\n",
    "                        aggregated_embedding = self.aggregate_op(self.word_embeddings[token.text], local_embedding)\n",
    "                        if self.pooling == 'fade':\n",
    "                            aggregated_embedding /= 2\n",
    "                        self.word_embeddings[token.text] = aggregated_embedding\n",
    "                        self.word_count[token.text] += 1\n",
    "\n",
    "        # add embeddings after updating\n",
    "        for sentence in sentences:\n",
    "            for token in sentence.tokens:\n",
    "                if token.text in self.word_embeddings:\n",
    "                    base = self.word_embeddings[token.text] / self.word_count[token.text] \\\n",
    "                        if self.pooling == 'mean' else self.word_embeddings[token.text]\n",
    "                else:\n",
    "                    base = token._embeddings[self.context_embeddings.name]\n",
    "\n",
    "                token.set_embedding(self.name, base)\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    def embedding_length(self) -> int:\n",
    "        return self.embedding_length\n",
    "\n",
    "\n",
    "class BertEmbeddings(TokenEmbeddings):\n",
    "\n",
    "    def __init__(self,\n",
    "                 bert_model: str = 'bert-base-uncased',\n",
    "                 layers: str = '-1,-2,-3,-4',\n",
    "                 pooling_operation: str = 'first'):\n",
    "        \"\"\"\n",
    "        Bidirectional transformer embeddings of words, as proposed in Devlin et al., 2018.\n",
    "        :param bert_model: name of BERT model ('')\n",
    "        :param layers: string indicating which layers to take for embedding\n",
    "        :param pooling_operation: how to get from token piece embeddings to token embedding. Either pool them and take\n",
    "        the average ('mean') or use first word piece embedding as token embedding ('first)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if bert_model not in PRETRAINED_MODEL_ARCHIVE_MAP.keys():\n",
    "            raise ValueError('Provided bert-model is not available.')\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "        self.model = BertModel.from_pretrained(bert_model)\n",
    "        self.layer_indexes = [int(x) for x in layers.split(\",\")]\n",
    "        self.pooling_operation = pooling_operation\n",
    "        self.name = str(bert_model)\n",
    "        self.static_embeddings = True\n",
    "\n",
    "    class BertInputFeatures(object):\n",
    "        \"\"\"Private helper class for holding BERT-formatted features\"\"\"\n",
    "\n",
    "        def __init__(self, unique_id, tokens, input_ids, input_mask, input_type_ids, token_subtoken_count):\n",
    "            self.unique_id = unique_id\n",
    "            self.tokens = tokens\n",
    "            self.input_ids = input_ids\n",
    "            self.input_mask = input_mask\n",
    "            self.input_type_ids = input_type_ids\n",
    "            self.token_subtoken_count = token_subtoken_count\n",
    "\n",
    "    def _convert_sentences_to_features(self, sentences, max_sequence_length: int) -> [BertInputFeatures]:\n",
    "\n",
    "        max_sequence_length = max_sequence_length + 2\n",
    "\n",
    "        features: List[BertEmbeddings.BertInputFeatures] = []\n",
    "        for (sentence_index, sentence) in enumerate(sentences):\n",
    "\n",
    "            bert_tokenization: List[str] = []\n",
    "            token_subtoken_count: Dict[int, int] = {}\n",
    "\n",
    "            for token in sentence:\n",
    "                subtokens = self.tokenizer.tokenize(token.text)\n",
    "                bert_tokenization.extend(subtokens)\n",
    "                token_subtoken_count[token.idx] = len(subtokens)\n",
    "\n",
    "            if len(bert_tokenization) > max_sequence_length - 2:\n",
    "                bert_tokenization = bert_tokenization[0:(max_sequence_length - 2)]\n",
    "\n",
    "            tokens = []\n",
    "            input_type_ids = []\n",
    "            tokens.append(\"[CLS]\")\n",
    "            input_type_ids.append(0)\n",
    "            for token in bert_tokenization:\n",
    "                tokens.append(token)\n",
    "                input_type_ids.append(0)\n",
    "            tokens.append(\"[SEP]\")\n",
    "            input_type_ids.append(0)\n",
    "\n",
    "            input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "            # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "            # tokens are attended to.\n",
    "            input_mask = [1] * len(input_ids)\n",
    "\n",
    "            # Zero-pad up to the sequence length.\n",
    "            while len(input_ids) < max_sequence_length:\n",
    "                input_ids.append(0)\n",
    "                input_mask.append(0)\n",
    "                input_type_ids.append(0)\n",
    "\n",
    "            features.append(BertEmbeddings.BertInputFeatures(\n",
    "                unique_id=sentence_index,\n",
    "                tokens=tokens,\n",
    "                input_ids=input_ids,\n",
    "                input_mask=input_mask,\n",
    "                input_type_ids=input_type_ids,\n",
    "                token_subtoken_count=token_subtoken_count))\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n",
    "        \"\"\"Add embeddings to all words in a list of sentences. If embeddings are already added,\n",
    "        updates only if embeddings are non-static.\"\"\"\n",
    "\n",
    "        # first, find longest sentence in batch\n",
    "        longest_sentence_in_batch: int = len(\n",
    "            max([self.tokenizer.tokenize(sentence.to_tokenized_string()) for sentence in sentences], key=len))\n",
    "\n",
    "        # prepare id maps for BERT model\n",
    "        features = self._convert_sentences_to_features(sentences, longest_sentence_in_batch)\n",
    "        all_input_ids = torch.LongTensor([f.input_ids for f in features]).to(flair.device)\n",
    "        all_input_masks = torch.LongTensor([f.input_mask for f in features]).to(flair.device)\n",
    "\n",
    "        # put encoded batch through BERT model to get all hidden states of all encoder layers\n",
    "        self.model.to(flair.device)\n",
    "        self.model.eval()\n",
    "        all_encoder_layers, _ = self.model(all_input_ids, token_type_ids=None, attention_mask=all_input_masks)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for sentence_index, sentence in enumerate(sentences):\n",
    "\n",
    "                feature = features[sentence_index]\n",
    "\n",
    "                # get aggregated embeddings for each BERT-subtoken in sentence\n",
    "                subtoken_embeddings = []\n",
    "                for token_index, _ in enumerate(feature.tokens):\n",
    "                    all_layers = []\n",
    "                    for layer_index in self.layer_indexes:\n",
    "                        layer_output = all_encoder_layers[int(layer_index)].detach().cpu()[sentence_index]\n",
    "                        all_layers.append(layer_output[token_index])\n",
    "\n",
    "                    subtoken_embeddings.append(torch.cat(all_layers))\n",
    "\n",
    "                # get the current sentence object\n",
    "                token_idx = 0\n",
    "                for token in sentence:\n",
    "                    # add concatenated embedding to sentence\n",
    "                    token_idx += 1\n",
    "\n",
    "                    if self.pooling_operation == 'first':\n",
    "                        # use first subword embedding if pooling operation is 'first'\n",
    "                        token.set_embedding(self.name, subtoken_embeddings[token_idx])\n",
    "                    else:\n",
    "                        # otherwise, do a mean over all subwords in token\n",
    "                        embeddings = subtoken_embeddings[token_idx:token_idx + feature.token_subtoken_count[token.idx]]\n",
    "                        embeddings = [embedding.unsqueeze(0) for embedding in embeddings]\n",
    "                        mean = torch.mean(torch.cat(embeddings, dim=0), dim=0)\n",
    "                        token.set_embedding(self.name, mean)\n",
    "\n",
    "                    token_idx += feature.token_subtoken_count[token.idx] - 1\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def embedding_length(self) -> int:\n",
    "        \"\"\"Returns the length of the embedding vector.\"\"\"\n",
    "        return len(self.layer_indexes) * self.model.config.hidden_size\n",
    "\n",
    "class CharLMEmbeddings(TokenEmbeddings):\n",
    "    \"\"\"Contextual string embeddings of words, as proposed in Akbik et al., 2018.\"\"\"\n",
    "\n",
    "    def __init__(self, model, detach: bool = True, use_cache: bool = True, cache_directory: str = None):\n",
    "        \"\"\"\n",
    "        initializes contextual string embeddings using a character-level language model.\n",
    "        :param model: model string, one of 'news-forward', 'news-backward', 'news-forward-fast', 'news-backward-fast',\n",
    "                'mix-forward', 'mix-backward', 'german-forward', 'german-backward', 'polish-backward', 'polish-forward'\n",
    "                depending on which character language model is desired.\n",
    "        :param detach: if set to False, the gradient will propagate into the language model. this dramatically slows down\n",
    "                training and often leads to worse results, so not recommended.\n",
    "        :param use_cache: if set to False, will not write embeddings to file for later retrieval. this saves disk space but will\n",
    "                not allow re-use of once computed embeddings that do not fit into memory\n",
    "        :param cache_directory: if cache_directory is not set, the cache will be written to ~/.flair/embeddings. otherwise the cache\n",
    "                is written to the provided directory.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # news-english-forward\n",
    "        if model.lower() == 'news-forward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-forward-v0.2rc.pt'\n",
    "            model = cached_path(base_path, cache_dir='embeddings')\n",
    "\n",
    "        # news-english-backward\n",
    "        elif model.lower() == 'news-backward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-backward-v0.2rc.pt'\n",
    "            model = cached_path(base_path, cache_dir='embeddings')\n",
    "\n",
    "        # news-english-forward\n",
    "        elif model.lower() == 'news-forward-fast':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-forward-1024-v0.2rc.pt'\n",
    "            model = cached_path(base_path, cache_dir='embeddings')\n",
    "\n",
    "        # news-english-backward\n",
    "        elif model.lower() == 'news-backward-fast':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-news-english-backward-1024-v0.2rc.pt'\n",
    "            model = cached_path(base_path, cache_dir='embeddings')\n",
    "\n",
    "        # mix-english-forward\n",
    "        elif model.lower() == 'mix-forward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-mix-english-forward-v0.2rc.pt'\n",
    "            model = cached_path(base_path, cache_dir='embeddings')\n",
    "\n",
    "        # mix-english-backward\n",
    "        elif model.lower() == 'mix-backward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-mix-english-backward-v0.2rc.pt'\n",
    "            model = cached_path(base_path, cache_dir='embeddings')\n",
    "\n",
    "        # mix-german-forward\n",
    "        elif model.lower() == 'german-forward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-mix-german-forward-v0.2rc.pt'\n",
    "            model = cached_path(base_path, cache_dir='embeddings')\n",
    "\n",
    "        # mix-german-backward\n",
    "        elif model.lower() == 'german-backward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-mix-german-backward-v0.2rc.pt'\n",
    "            model = cached_path(base_path, cache_dir='embeddings')\n",
    "\n",
    "        # common crawl Polish forward\n",
    "        elif model.lower() == 'polish-forward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-polish-forward-v0.2.pt'\n",
    "            model = cached_path(base_path, cache_dir='embeddings')\n",
    "\n",
    "        # common crawl Polish backward\n",
    "        elif model.lower() == 'polish-backward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings/lm-polish-backward-v0.2.pt'\n",
    "            model = cached_path(base_path, cache_dir='embeddings')\n",
    "\n",
    "        # Slovenian forward\n",
    "        elif model.lower() == 'slovenian-forward' or model.lower() == 'sl-forward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.3/lm-sl-large-forward-v0.1.pt'\n",
    "            model = cached_path(base_path, cache_dir='embeddings')\n",
    "        # Slovenian backward\n",
    "        elif model.lower() == 'slovenian-backward' or model.lower() == 'sl-backward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.3/lm-sl-large-backward-v0.1.pt'\n",
    "            model = cached_path(base_path, cache_dir='embeddings')\n",
    "\n",
    "        # Bulgarian forward\n",
    "        elif model.lower() == 'bulgarian-forward' or model.lower() == 'bg-forward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.3/lm-bg-small-forward-v0.1.pt'\n",
    "            model = cached_path(base_path, cache_dir='embeddings')\n",
    "        # Bulgarian backward\n",
    "        elif model.lower() == 'bulgarian-backward' or model.lower() == 'bg-backward':\n",
    "            base_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.3/lm-bg-small-backward-v0.1.pt'\n",
    "            model = cached_path(base_path, cache_dir='embeddings')\n",
    "        elif model.lower() == \"pubmed-forward\":\n",
    "            base_path = \"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4.1/pubmed-2015-fw-lm.pt\"\n",
    "            model = cached_path(base_path, cache_dir='embeddings')\n",
    "        # Pubmed backward\n",
    "        elif model.lower() == \"pubmed-backward\":\n",
    "            base_path = \"https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/embeddings-v0.4.1/pubmed-2015-bw-lm.pt\"\n",
    "            model = cached_path(base_path, cache_dir='embeddings')\n",
    "        elif not os.path.exists(model):\n",
    "            raise ValueError(f'The given model \"{model}\" is not available or is not a valid path.')\n",
    "\n",
    "        self.name = model\n",
    "        self.static_embeddings = detach\n",
    "\n",
    "        from flair.models import LanguageModel\n",
    "        self.lm = LanguageModel.load_language_model(model)\n",
    "        self.detach = detach\n",
    "\n",
    "        self.is_forward_lm: bool = self.lm.is_forward_lm\n",
    "\n",
    "        # caching variables\n",
    "        self.use_cache: bool = use_cache\n",
    "        self.cache = None\n",
    "        self.cache_directory: str = cache_directory\n",
    "\n",
    "        dummy_sentence: Sentence = Sentence()\n",
    "        dummy_sentence.add_token(Token('hello'))\n",
    "        embedded_dummy = self.embed(dummy_sentence)\n",
    "        self.__embedding_length: int = len(embedded_dummy[0].get_token(1).get_embedding())\n",
    "\n",
    "        # set to eval mode\n",
    "        self.eval()\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        pass\n",
    "\n",
    "    def __getstate__(self):\n",
    "        # Copy the object's state from self.__dict__ which contains\n",
    "        # all our instance attributes. Always use the dict.copy()\n",
    "        # method to avoid modifying the original state.\n",
    "        state = self.__dict__.copy()\n",
    "        # Remove the unpicklable entries.\n",
    "        state['cache'] = None\n",
    "        state['use_cache'] = False\n",
    "        state['cache_directory'] = None\n",
    "        return state\n",
    "\n",
    "    @property\n",
    "    def embedding_length(self) -> int:\n",
    "        return self.__embedding_length\n",
    "\n",
    "    def _add_embeddings_internal(self, sentences: List[Sentence]) -> List[Sentence]:\n",
    "\n",
    "        # this whole block is for compatibility with older serialized models  TODO: remove in version 0.4\n",
    "        if 'cache' not in self.__dict__ or 'cache_directory' not in self.__dict__:\n",
    "            self.use_cache = False\n",
    "            self.cache_directory = None\n",
    "        else:\n",
    "            cache_path = '{}-tmp-cache.sqllite'.format(self.name) if not self.cache_directory else os.path.join(\n",
    "                self.cache_directory, '{}-tmp-cache.sqllite'.format(os.path.basename(self.name)))\n",
    "            if not os.path.exists(cache_path):\n",
    "                self.use_cache = False\n",
    "                self.cache_directory = None\n",
    "\n",
    "        # if cache is used, try setting embeddings from cache first\n",
    "        if self.use_cache:\n",
    "\n",
    "            # lazy initialization of cache\n",
    "            if not self.cache:\n",
    "                from sqlitedict import SqliteDict\n",
    "                self.cache = SqliteDict(cache_path, autocommit=True)\n",
    "\n",
    "            # try populating embeddings from cache\n",
    "            all_embeddings_retrieved_from_cache: bool = True\n",
    "            for sentence in sentences:\n",
    "                key = sentence.to_tokenized_string()\n",
    "                embeddings = self.cache.get(key)\n",
    "\n",
    "                if not embeddings:\n",
    "                    all_embeddings_retrieved_from_cache = False\n",
    "                    break\n",
    "                else:\n",
    "                    for token, embedding in zip(sentence, embeddings):\n",
    "                        token.set_embedding(self.name, torch.FloatTensor(embedding))\n",
    "\n",
    "            if all_embeddings_retrieved_from_cache:\n",
    "                return sentences\n",
    "\n",
    "        # if this is not possible, use LM to generate embedding. First, get text sentences\n",
    "        text_sentences = [sentence.to_tokenized_string() for sentence in sentences]\n",
    "\n",
    "        longest_character_sequence_in_batch: int = len(max(text_sentences, key=len))\n",
    "\n",
    "        # pad strings with whitespaces to longest sentence\n",
    "        sentences_padded: List[str] = []\n",
    "        append_padded_sentence = sentences_padded.append\n",
    "\n",
    "        end_marker = ' '\n",
    "        extra_offset = 1\n",
    "        for sentence_text in text_sentences:\n",
    "            pad_by = longest_character_sequence_in_batch - len(sentence_text)\n",
    "            if self.is_forward_lm:\n",
    "                padded = '\\n{}{}{}'.format(sentence_text, end_marker, pad_by * ' ')\n",
    "                append_padded_sentence(padded)\n",
    "            else:\n",
    "                padded = '\\n{}{}{}'.format(sentence_text[::-1], end_marker, pad_by * ' ')\n",
    "                append_padded_sentence(padded)\n",
    "\n",
    "        # get hidden states from language model\n",
    "        all_hidden_states_in_lm = self.lm.get_representation(sentences_padded, self.detach)\n",
    "\n",
    "        # take first or last hidden states from language model as word representation\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sentence_text = sentence.to_tokenized_string()\n",
    "\n",
    "            offset_forward: int = extra_offset\n",
    "            offset_backward: int = len(sentence_text) + extra_offset\n",
    "\n",
    "            for token in sentence.tokens:\n",
    "                token: Token = token\n",
    "\n",
    "                offset_forward += len(token.text)\n",
    "\n",
    "                if self.is_forward_lm:\n",
    "                    offset = offset_forward\n",
    "                else:\n",
    "                    offset = offset_backward\n",
    "\n",
    "                embedding = all_hidden_states_in_lm[offset, i, :]\n",
    "\n",
    "                # if self.tokenized_lm or token.whitespace_after:\n",
    "                offset_forward += 1\n",
    "                offset_backward -= 1\n",
    "\n",
    "                offset_backward -= len(token.text)\n",
    "\n",
    "                token.set_embedding(self.name, embedding)\n",
    "\n",
    "        if self.use_cache:\n",
    "            for sentence in sentences:\n",
    "                self.cache[sentence.to_tokenized_string()] = [token._embeddings[self.name].tolist() for token in\n",
    "                                                              sentence]\n",
    "\n",
    "        return sentences\n",
    "\n",
    "\n",
    "class DocumentMeanEmbeddings(DocumentEmbeddings):\n",
    "\n",
    "    @deprecated(version='0.3.1', reason=\"The functionality of this class is moved to 'DocumentPoolEmbeddings'\")\n",
    "    def __init__(self, token_embeddings: List[TokenEmbeddings]):\n",
    "        \"\"\"The constructor takes a list of embeddings to be combined.\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=token_embeddings)\n",
    "        self.name: str = 'document_mean'\n",
    "\n",
    "        self.__embedding_length: int = self.embeddings.embedding_length\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "\n",
    "    @property\n",
    "    def embedding_length(self) -> int:\n",
    "        return self.__embedding_length\n",
    "\n",
    "    def embed(self, sentences: Union[List[Sentence], Sentence]):\n",
    "        \"\"\"Add embeddings to every sentence in the given list of sentences. If embeddings are already added, updates\n",
    "        only if embeddings are non-static.\"\"\"\n",
    "\n",
    "        everything_embedded: bool = True\n",
    "\n",
    "        # if only one sentence is passed, convert to list of sentence\n",
    "        if type(sentences) is Sentence:\n",
    "            sentences = [sentences]\n",
    "\n",
    "        for sentence in sentences:\n",
    "            if self.name not in sentence._embeddings.keys(): everything_embedded = False\n",
    "\n",
    "        if not everything_embedded:\n",
    "\n",
    "            self.embeddings.embed(sentences)\n",
    "\n",
    "            for sentence in sentences:\n",
    "                word_embeddings = []\n",
    "                for token in sentence.tokens:\n",
    "                    token: Token = token\n",
    "                    word_embeddings.append(token.get_embedding().unsqueeze(0))\n",
    "\n",
    "                word_embeddings = torch.cat(word_embeddings, dim=0)\n",
    "                if torch.cuda.is_available():\n",
    "                    word_embeddings = word_embeddings.cuda()\n",
    "\n",
    "                mean_embedding = torch.mean(word_embeddings, 0)\n",
    "\n",
    "                sentence.set_embedding(self.name, mean_embedding.unsqueeze(0))\n",
    "\n",
    "    def _add_embeddings_internal(self, sentences: List[Sentence]):\n",
    "        pass\n",
    "\n",
    "\n",
    "class DocumentPoolEmbeddings(DocumentEmbeddings):\n",
    "\n",
    "    def __init__(self, token_embeddings: List[TokenEmbeddings], mode: str = 'mean'):\n",
    "        \"\"\"The constructor takes a list of embeddings to be combined.\n",
    "        :param token_embeddings: a list of token embeddings\n",
    "        :param mode: a string which can any value from ['mean', 'max', 'min']\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=token_embeddings)\n",
    "\n",
    "        self.__embedding_length: int = self.embeddings.embedding_length\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "\n",
    "        self.mode = mode\n",
    "        if self.mode == 'mean':\n",
    "            self.pool_op = torch.mean\n",
    "        elif mode == 'max':\n",
    "            self.pool_op = torch.max\n",
    "        elif mode == 'min':\n",
    "            self.pool_op = torch.min\n",
    "        else:\n",
    "            raise ValueError(f'Pooling operation for {self.mode!r} is not defined')\n",
    "        self.name: str = f'document_{self.mode}'\n",
    "\n",
    "    @property\n",
    "    def embedding_length(self) -> int:\n",
    "        return self.__embedding_length\n",
    "\n",
    "    def embed(self, sentences: Union[List[Sentence], Sentence]):\n",
    "        \"\"\"Add embeddings to every sentence in the given list of sentences. If embeddings are already added, updates\n",
    "        only if embeddings are non-static.\"\"\"\n",
    "\n",
    "        everything_embedded: bool = True\n",
    "\n",
    "        # if only one sentence is passed, convert to list of sentence\n",
    "        if isinstance(sentences, Sentence):\n",
    "            sentences = [sentences]\n",
    "\n",
    "        for sentence in sentences:\n",
    "            if self.name not in sentence._embeddings.keys(): everything_embedded = False\n",
    "\n",
    "        if not everything_embedded:\n",
    "\n",
    "            self.embeddings.embed(sentences)\n",
    "\n",
    "            for sentence in sentences:\n",
    "                word_embeddings = []\n",
    "                for token in sentence.tokens:\n",
    "                    token: Token = token\n",
    "                    word_embeddings.append(token.get_embedding().unsqueeze(0))\n",
    "\n",
    "                word_embeddings = torch.cat(word_embeddings, dim=0)\n",
    "                if torch.cuda.is_available():\n",
    "                    word_embeddings = word_embeddings.cuda()\n",
    "\n",
    "                if self.mode == 'mean':\n",
    "                    pooled_embedding = self.pool_op(word_embeddings, 0)\n",
    "                else:\n",
    "                    pooled_embedding, _ = self.pool_op(word_embeddings, 0)\n",
    "\n",
    "                sentence.set_embedding(self.name, pooled_embedding.unsqueeze(0))\n",
    "\n",
    "    def _add_embeddings_internal(self, sentences: List[Sentence]):\n",
    "        pass\n",
    "\n",
    "\n",
    "class DocumentLSTMEmbeddings(DocumentEmbeddings):\n",
    "\n",
    "    def __init__(self,\n",
    "                 token_embeddings: List[TokenEmbeddings],\n",
    "                 hidden_states=128,\n",
    "                 num_layers=1,\n",
    "                 reproject_words: bool = True,\n",
    "                 reproject_words_dimension: int = None,\n",
    "                 bidirectional: bool = False,\n",
    "                 use_word_dropout: bool = False,\n",
    "                 use_locked_dropout: bool = False):\n",
    "        \"\"\"The constructor takes a list of embeddings to be combined.\n",
    "        :param token_embeddings: a list of token embeddings\n",
    "        :param hidden_states: the number of hidden states in the lstm\n",
    "        :param num_layers: the number of layers for the lstm\n",
    "        :param reproject_words: boolean value, indicating whether to reproject the token embeddings in a separate linear\n",
    "        layer before putting them into the lstm or not\n",
    "        :param reproject_words_dimension: output dimension of reprojecting token embeddings. If None the same output\n",
    "        dimension as before will be taken.\n",
    "        :param bidirectional: boolean value, indicating whether to use a bidirectional lstm or not\n",
    "        representation of the lstm to be used as final document embedding.\n",
    "        :param use_word_dropout: boolean value, indicating whether to use word dropout or not.\n",
    "        :param use_locked_dropout: boolean value, indicating whether to use locked dropout or not.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=token_embeddings)\n",
    "\n",
    "        self.reproject_words = reproject_words\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        self.length_of_all_token_embeddings: int = self.embeddings.embedding_length\n",
    "\n",
    "        self.name = 'document_lstm'\n",
    "        self.static_embeddings = False\n",
    "\n",
    "        self.__embedding_length: int = hidden_states\n",
    "        if self.bidirectional:\n",
    "            self.__embedding_length *= 4\n",
    "\n",
    "        self.embeddings_dimension: int = self.length_of_all_token_embeddings\n",
    "        if self.reproject_words and reproject_words_dimension is not None:\n",
    "            self.embeddings_dimension = reproject_words_dimension\n",
    "\n",
    "        # bidirectional LSTM on top of embedding layer\n",
    "        self.word_reprojection_map = torch.nn.Linear(self.length_of_all_token_embeddings,\n",
    "                                                     self.embeddings_dimension)\n",
    "        self.rnn = torch.nn.GRU(self.embeddings_dimension, hidden_states, num_layers=num_layers,\n",
    "                                bidirectional=self.bidirectional)\n",
    "\n",
    "        # dropouts\n",
    "        if use_locked_dropout:\n",
    "            self.dropout: torch.nn.Module = LockedDropout(0.5)\n",
    "        else:\n",
    "            self.dropout = torch.nn.Dropout(0.5)\n",
    "\n",
    "        self.use_word_dropout: bool = use_word_dropout\n",
    "        if self.use_word_dropout:\n",
    "            self.word_dropout = WordDropout(0.05)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.word_reprojection_map.weight)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "\n",
    "    @property\n",
    "    def embedding_length(self) -> int:\n",
    "        return self.__embedding_length\n",
    "\n",
    "    def embed(self, sentences: Union[List[Sentence], Sentence]):\n",
    "        \"\"\"Add embeddings to all sentences in the given list of sentences. If embeddings are already added, update\n",
    "         only if embeddings are non-static.\"\"\"\n",
    "\n",
    "        if type(sentences) is Sentence:\n",
    "            sentences = [sentences]\n",
    "\n",
    "        self.rnn.zero_grad()\n",
    "\n",
    "        sentences.sort(key=lambda x: len(x), reverse=True)\n",
    "\n",
    "        self.embeddings.embed(sentences)\n",
    "\n",
    "        # first, sort sentences by number of tokens\n",
    "        longest_token_sequence_in_batch: int = len(sentences[0])\n",
    "\n",
    "        all_sentence_tensors = []\n",
    "        lengths: List[int] = []\n",
    "\n",
    "        # go through each sentence in batch\n",
    "        for i, sentence in enumerate(sentences):\n",
    "\n",
    "            lengths.append(len(sentence.tokens))\n",
    "\n",
    "            word_embeddings = []\n",
    "\n",
    "            for token, token_idx in zip(sentence.tokens, range(len(sentence.tokens))):\n",
    "                token: Token = token\n",
    "                word_embeddings.append(token.get_embedding().unsqueeze(0))\n",
    "\n",
    "            # PADDING: pad shorter sentences out\n",
    "            for add in range(longest_token_sequence_in_batch - len(sentence.tokens)):\n",
    "                word_embeddings.append(\n",
    "                    torch.FloatTensor(np.zeros(self.length_of_all_token_embeddings, dtype='float')).unsqueeze(0))\n",
    "\n",
    "            word_embeddings_tensor = torch.cat(word_embeddings, 0)\n",
    "\n",
    "            sentence_states = word_embeddings_tensor\n",
    "\n",
    "            # ADD TO SENTENCE LIST: add the representation\n",
    "            all_sentence_tensors.append(sentence_states.unsqueeze(1))\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # GET REPRESENTATION FOR ENTIRE BATCH\n",
    "        # --------------------------------------------------------------------\n",
    "        sentence_tensor = torch.cat(all_sentence_tensors, 1)\n",
    "        if torch.cuda.is_available():\n",
    "            sentence_tensor = sentence_tensor.cuda()\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # FF PART\n",
    "        # --------------------------------------------------------------------\n",
    "        # use word dropout if set\n",
    "        if self.use_word_dropout:\n",
    "            sentence_tensor = self.word_dropout(sentence_tensor)\n",
    "\n",
    "        if self.reproject_words:\n",
    "            sentence_tensor = self.word_reprojection_map(sentence_tensor)\n",
    "\n",
    "        sentence_tensor = self.dropout(sentence_tensor)\n",
    "\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(sentence_tensor, lengths)\n",
    "\n",
    "        self.rnn.flatten_parameters()\n",
    "\n",
    "        lstm_out, hidden = self.rnn(packed)\n",
    "\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(lstm_out)\n",
    "\n",
    "        outputs = self.dropout(outputs)\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # EXTRACT EMBEDDINGS FROM LSTM\n",
    "        # --------------------------------------------------------------------\n",
    "        for sentence_no, length in enumerate(lengths):\n",
    "            last_rep = outputs[length - 1, sentence_no].unsqueeze(0)\n",
    "\n",
    "            embedding = last_rep\n",
    "            if self.bidirectional:\n",
    "                first_rep = outputs[0, sentence_no].unsqueeze(0)\n",
    "                embedding = torch.cat([first_rep, last_rep], 1)\n",
    "\n",
    "            sentence = sentences[sentence_no]\n",
    "            sentence.set_embedding(self.name, embedding)\n",
    "\n",
    "    def _add_embeddings_internal(self, sentences: List[Sentence]):\n",
    "        pass\n",
    "\n",
    "\n",
    "class DocumentLMEmbeddings(DocumentEmbeddings):\n",
    "    def __init__(self, charlm_embeddings: List[CharLMEmbeddings], detach: bool = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = charlm_embeddings\n",
    "\n",
    "        self.static_embeddings = detach\n",
    "        self.detach = detach\n",
    "\n",
    "        dummy: Sentence = Sentence('jo')\n",
    "        self.embed([dummy])\n",
    "        self._embedding_length: int = len(dummy.embedding)\n",
    "\n",
    "    @property\n",
    "    def embedding_length(self) -> int:\n",
    "        return self._embedding_length\n",
    "\n",
    "    def embed(self, sentences: Union[List[Sentence], Sentence]):\n",
    "        if type(sentences) is Sentence:\n",
    "            sentences = [sentences]\n",
    "\n",
    "        for embedding in self.embeddings:\n",
    "            embedding.embed(sentences)\n",
    "\n",
    "            # iterate over sentences\n",
    "            for sentence in sentences:\n",
    "\n",
    "                # if its a forward LM, take last state\n",
    "                if embedding.is_forward_lm:\n",
    "                    sentence.set_embedding(embedding.name, sentence[len(sentence)]._embeddings[embedding.name])\n",
    "                else:\n",
    "                    sentence.set_embedding(embedding.name, sentence[1]._embeddings[embedding.name])\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    def _add_embeddings_internal(self, sentences: List[Sentence]):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "from typing import List\n",
    "#from flair.data import Dictionary\n",
    "\n",
    "\n",
    "class LanguageModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dictionary: Dictionary,\n",
    "                 is_forward_lm: bool,\n",
    "                 hidden_size: int,\n",
    "                 nlayers: int,\n",
    "                 embedding_size: int = 100,\n",
    "                 nout=None,\n",
    "                 dropout=0.5,\n",
    "                 best_score=None):\n",
    "\n",
    "        super(LanguageModel, self).__init__()\n",
    "\n",
    "        self.dictionary = dictionary\n",
    "        self.is_forward_lm: bool = is_forward_lm\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(len(dictionary), embedding_size)\n",
    "\n",
    "        if nlayers == 1:\n",
    "            self.rnn = nn.LSTM(embedding_size, hidden_size, nlayers)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(embedding_size, hidden_size, nlayers, dropout=dropout)\n",
    "\n",
    "        self.hidden = None\n",
    "\n",
    "        self.nout = nout\n",
    "        if nout is not None:\n",
    "            self.proj = nn.Linear(hidden_size, nout)\n",
    "            self.initialize(self.proj.weight)\n",
    "            self.decoder = nn.Linear(nout, len(dictionary))\n",
    "        else:\n",
    "            self.proj = None\n",
    "            self.decoder = nn.Linear(hidden_size, len(dictionary))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.best_score = best_score\n",
    "\n",
    "        # auto-spawn on GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def set_hidden(self, hidden):\n",
    "        self.hidden = hidden\n",
    "\n",
    "    def forward(self, input, hidden, ordered_sequence_lengths=None):\n",
    "        encoded = self.encoder(input)\n",
    "        emb = self.drop(encoded)\n",
    "\n",
    "        self.rnn.flatten_parameters()\n",
    "\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "\n",
    "        if self.proj is not None:\n",
    "            output = self.proj(output)\n",
    "\n",
    "        output = self.drop(output)\n",
    "\n",
    "        decoded = self.decoder(output.view(output.size(0) * output.size(1), output.size(2)))\n",
    "\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), output, hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (Variable(weight.new(self.nlayers, bsz, self.hidden_size).zero_()),\n",
    "                Variable(weight.new(self.nlayers, bsz, self.hidden_size).zero_()))\n",
    "\n",
    "    def get_representation(self, strings: List[str], detach_from_lm=True):\n",
    "\n",
    "        sequences_as_char_indices: List[List[int]] = []\n",
    "        for string in strings:\n",
    "            char_indices = [self.dictionary.get_idx_for_item(char) for char in string]\n",
    "            sequences_as_char_indices.append(char_indices)\n",
    "\n",
    "        batch = Variable(torch.LongTensor(sequences_as_char_indices).transpose(0, 1))\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            batch = batch.cuda()\n",
    "\n",
    "        hidden = self.init_hidden(len(strings))\n",
    "        prediction, rnn_output, hidden = self.forward(batch, hidden)\n",
    "\n",
    "        if detach_from_lm: rnn_output = self.repackage_hidden(rnn_output)\n",
    "\n",
    "        return rnn_output\n",
    "\n",
    "    def repackage_hidden(self, h):\n",
    "        \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "        if type(h) == torch.Tensor:\n",
    "            return Variable(h.data)\n",
    "        else:\n",
    "            return tuple(self.repackage_hidden(v) for v in h)\n",
    "\n",
    "    def initialize(self, matrix):\n",
    "        in_, out_ = matrix.size()\n",
    "        stdv = math.sqrt(3. / (in_ + out_))\n",
    "        matrix.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    @classmethod\n",
    "    def load_language_model(cls, model_file):\n",
    "\n",
    "        if not torch.cuda.is_available():\n",
    "            state = torch.load(model_file, map_location='cpu')\n",
    "        else:\n",
    "            state = torch.load(model_file)\n",
    "\n",
    "        best_score = state['best_score'] if 'best_score' in state else None\n",
    "\n",
    "        model = LanguageModel(state['dictionary'],\n",
    "                                             state['is_forward_lm'],\n",
    "                                             state['hidden_size'],\n",
    "                                             state['nlayers'],\n",
    "                                             state['embedding_size'],\n",
    "                                             state['nout'],\n",
    "                                             state['dropout'],\n",
    "                                             best_score)\n",
    "        model.load_state_dict(state['state_dict'])\n",
    "        model.eval()\n",
    "        if torch.cuda.is_available():\n",
    "            model.cuda()\n",
    "        return model\n",
    "\n",
    "    def save(self, file):\n",
    "        model_state = {\n",
    "            'state_dict': self.state_dict(),\n",
    "            'dictionary': self.dictionary,\n",
    "            'is_forward_lm': self.is_forward_lm,\n",
    "            'hidden_size': self.hidden_size,\n",
    "            'nlayers': self.nlayers,\n",
    "            'embedding_size': self.embedding_size,\n",
    "            'nout': self.nout,\n",
    "            'dropout': self.dropout,\n",
    "            'best_score': self.best_score\n",
    "        }\n",
    "        torch.save(model_state, file, pickle_protocol=4)\n",
    "\n",
    "    def generate_text(self, number_of_characters=1000) -> str:\n",
    "        with torch.no_grad():\n",
    "            characters = []\n",
    "\n",
    "            idx2item = self.dictionary.idx2item\n",
    "\n",
    "            # initial hidden state\n",
    "            hidden = self.init_hidden(1)\n",
    "            input = torch.rand(1, 1).mul(len(idx2item)).long()\n",
    "            if torch.cuda.is_available():\n",
    "                input = input.cuda()\n",
    "\n",
    "            for i in range(number_of_characters):\n",
    "                prediction, rnn_output, hidden = self.forward(input, hidden)\n",
    "                word_weights = prediction.squeeze().data.div(1.0).exp().cpu()\n",
    "                word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "                input.data.fill_(word_idx)\n",
    "                word = idx2item[word_idx].decode('UTF-8')\n",
    "                characters.append(word)\n",
    "\n",
    "            return ''.join(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiny-tokenizer in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiny-tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flair in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.4.4)\n",
      "Requirement already satisfied: gensim>=3.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (3.8.1)\n",
      "Requirement already satisfied: tabulate in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (0.8.6)\n",
      "Requirement already satisfied: segtok>=1.5.7 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (1.5.7)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (3.0.3)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (0.4.0a0+6b959ee)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (4.36.1)\n",
      "Requirement already satisfied: ipython-genutils==0.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (0.2.0)\n",
      "Requirement already satisfied: hyperopt>=0.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (0.2.2)\n",
      "Requirement already satisfied: bpemb>=0.2.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (0.3.0)\n",
      "Requirement already satisfied: pytest>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (5.3.2)\n",
      "Requirement already satisfied: sklearn in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (0.0)\n",
      "Requirement already satisfied: sqlitedict>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (1.6.0)\n",
      "Requirement already satisfied: langdetect in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (1.0.7)\n",
      "Requirement already satisfied: pymongo in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (3.10.0)\n",
      "Requirement already satisfied: ipython==7.6.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (7.6.1)\n",
      "Requirement already satisfied: deprecated>=1.2.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (1.2.7)\n",
      "Requirement already satisfied: mpld3==0.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (0.3)\n",
      "Requirement already satisfied: regex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (2019.12.20)\n",
      "Requirement already satisfied: tiny-tokenizer[all] in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (3.1.0)\n",
      "Requirement already satisfied: transformers>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (2.3.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.20 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (1.23)\n",
      "Requirement already satisfied: torch>=1.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from flair) (1.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from gensim>=3.4.0->flair) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from gensim>=3.4.0->flair) (1.15.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from gensim>=3.4.0->flair) (1.9.0)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from gensim>=3.4.0->flair) (1.11.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib>=2.2.3->flair) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib>=2.2.3->flair) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from matplotlib>=2.2.3->flair) (2.7.3)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision->flair) (5.2.0)\n",
      "Requirement already satisfied: cloudpickle in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from hyperopt>=0.1.1->flair) (0.5.3)\n",
      "Requirement already satisfied: networkx==2.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from hyperopt>=0.1.1->flair) (2.2)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from hyperopt>=0.1.1->flair) (0.18.2)\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from bpemb>=0.2.9->flair) (0.1.85)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from bpemb>=0.2.9->flair) (2.20.0)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytest>=3.6.4->flair) (4.1.0)\n",
      "Requirement already satisfied: pluggy<1.0,>=0.12 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytest>=3.6.4->flair) (0.13.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytest>=3.6.4->flair) (1.3.0)\n",
      "Requirement already satisfied: wcwidth in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytest>=3.6.4->flair) (0.1.7)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytest>=3.6.4->flair) (17.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytest>=3.6.4->flair) (18.1.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pytest>=3.6.4->flair) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sklearn->flair) (0.20.3)\n",
      "Requirement already satisfied: backcall in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython==7.6.1->flair) (0.1.0)\n",
      "Requirement already satisfied: decorator in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython==7.6.1->flair) (4.3.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython==7.6.1->flair) (39.1.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython==7.6.1->flair) (4.3.2)\n",
      "Requirement already satisfied: pygments in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython==7.6.1->flair) (2.2.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython==7.6.1->flair) (2.0.10)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython==7.6.1->flair) (4.5.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython==7.6.1->flair) (0.12.0)\n",
      "Requirement already satisfied: pickleshare in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from ipython==7.6.1->flair) (0.7.4)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from deprecated>=1.2.4->flair) (1.10.11)\n",
      "Requirement already satisfied: kytea; extra == \"all\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tiny-tokenizer[all]->flair) (0.1.4)\n",
      "Requirement already satisfied: natto-py; extra == \"all\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tiny-tokenizer[all]->flair) (0.9.2)\n",
      "Requirement already satisfied: SudachiDict-core; extra == \"all\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tiny-tokenizer[all]->flair) (0.0.0)\n",
      "Requirement already satisfied: SudachiPy; extra == \"all\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tiny-tokenizer[all]->flair) (0.4.2)\n",
      "Requirement already satisfied: janome; extra == \"all\" in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tiny-tokenizer[all]->flair) (0.3.10)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=2.0.0->flair) (0.0.35)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=2.0.0->flair) (1.10.19)\n",
      "Requirement already satisfied: boto>=2.32 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim>=3.4.0->flair) (2.48.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->bpemb>=0.2.9->flair) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->bpemb>=0.2.9->flair) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->bpemb>=0.2.9->flair) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=3.6.4->flair) (0.6.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->ipython==7.6.1->flair) (0.5.2)\n",
      "Requirement already satisfied: parso>=0.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from jedi>=0.10->ipython==7.6.1->flair) (0.2.0)\n",
      "Requirement already satisfied: cffi in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from natto-py; extra == \"all\"->tiny-tokenizer[all]->flair) (1.11.5)\n",
      "Requirement already satisfied: dartsclone~=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from SudachiPy; extra == \"all\"->tiny-tokenizer[all]->flair) (0.6)\n",
      "Requirement already satisfied: sortedcontainers~=2.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from SudachiPy; extra == \"all\"->tiny-tokenizer[all]->flair) (2.1.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers>=2.0.0->flair) (6.7)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers>=2.0.0->flair) (0.14.1)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->transformers>=2.0.0->flair) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->transformers>=2.0.0->flair) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.19 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3->transformers>=2.0.0->flair) (1.13.19)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from cffi->natto-py; extra == \"all\"->tiny-tokenizer[all]->flair) (2.18)\n",
      "Requirement already satisfied: Cython in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from dartsclone~=0.6.0->SudachiPy; extra == \"all\"->tiny-tokenizer[all]->flair) (0.28.2)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore<1.14.0,>=1.13.19->boto3->transformers>=2.0.0->flair) (0.14)\n"
     ]
    }
   ],
   "source": [
    "#!pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "import torch.autograd as autograd\n",
    "import torch.nn\n",
    "import flair.nn\n",
    "import torch\n",
    "\n",
    "#import flair.embeddings\n",
    "#from flair.data import Dictionary, Sentence, Token\n",
    "#from flair.file_utils import cached_path\n",
    "\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "#from flair.training_utils import clear_embeddings\n",
    "\n",
    "import os\n",
    "current_path = os.getcwd() + \"/ner/\"\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "START_TAG: str = '<START>'\n",
    "STOP_TAG: str = '<STOP>'\n",
    "\n",
    "\n",
    "def to_scalar(var):\n",
    "    return var.view(-1).data.tolist()[0]\n",
    "\n",
    "\n",
    "def argmax(vec):\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return to_scalar(idx)\n",
    "\n",
    "\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "           torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "\n",
    "def argmax_batch(vecs):\n",
    "    _, idx = torch.max(vecs, 1)\n",
    "    return idx\n",
    "\n",
    "\n",
    "def log_sum_exp_batch(vecs):\n",
    "    maxi = torch.max(vecs, 1)[0]\n",
    "    maxi_bc = maxi[:, None].repeat(1, vecs.shape[1])\n",
    "    recti_ = torch.log(torch.sum(torch.exp(vecs - maxi_bc), 1))\n",
    "    return maxi + recti_\n",
    "\n",
    "\n",
    "def pad_tensors(tensor_list, type_=torch.FloatTensor):\n",
    "    ml = max([x.shape[0] for x in tensor_list])\n",
    "    shape = [len(tensor_list), ml] + list(tensor_list[0].shape[1:])\n",
    "    template = type_(*shape)\n",
    "    template.fill_(0)\n",
    "    lens_ = [x.shape[0] for x in tensor_list]\n",
    "    for i, tensor in enumerate(tensor_list):\n",
    "        template[i, :lens_[i]] = tensor\n",
    "\n",
    "    return template, lens_\n",
    "\n",
    "\n",
    "class SequenceTagger(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 embeddings: flair.embeddings.TokenEmbeddings,\n",
    "                 tag_dictionary: Dictionary,\n",
    "                 tag_type: str,\n",
    "                 use_crf: bool = True,\n",
    "                 use_rnn: bool = True,\n",
    "                 rnn_layers: int = 1,\n",
    "                 use_dropout: float = 0.0,\n",
    "                 use_word_dropout: float = 0.05,\n",
    "                 use_locked_dropout: float = 0.5,\n",
    "                 ):\n",
    "\n",
    "        super(SequenceTagger, self).__init__()\n",
    "\n",
    "        self.use_rnn = use_rnn\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_crf: bool = use_crf\n",
    "        self.rnn_layers: int = rnn_layers\n",
    "\n",
    "        self.trained_epochs: int = 0\n",
    "\n",
    "        self.embeddings = embeddings\n",
    "\n",
    "        # set the dictionaries\n",
    "        self.tag_dictionary: Dictionary = tag_dictionary\n",
    "        self.tag_type: str = tag_type\n",
    "        self.tagset_size: int = len(tag_dictionary)\n",
    "\n",
    "        # initialize the network architecture\n",
    "        self.nlayers: int = rnn_layers\n",
    "        self.hidden_word = None\n",
    "\n",
    "        # dropouts\n",
    "        self.use_dropout: float = use_dropout\n",
    "        self.use_word_dropout: float = use_word_dropout\n",
    "        self.use_locked_dropout: float = use_locked_dropout\n",
    "\n",
    "        if use_dropout > 0.0:\n",
    "            self.dropout = torch.nn.Dropout(use_dropout)\n",
    "\n",
    "        if use_word_dropout > 0.0:\n",
    "            self.word_dropout = flair.nn.WordDropout(use_word_dropout)\n",
    "\n",
    "        if use_locked_dropout > 0.0:\n",
    "            self.locked_dropout = flair.nn.LockedDropout(use_locked_dropout)\n",
    "\n",
    "        rnn_input_dim: int = self.embeddings.embedding_length\n",
    "\n",
    "        self.relearn_embeddings: bool = True\n",
    "\n",
    "        if self.relearn_embeddings:\n",
    "            self.embedding2nn = torch.nn.Linear(rnn_input_dim, rnn_input_dim)\n",
    "\n",
    "        # bidirectional LSTM on top of embedding layer\n",
    "        self.rnn_type = 'LSTM'\n",
    "        if self.rnn_type in ['LSTM', 'GRU']:\n",
    "\n",
    "            if self.nlayers == 1:\n",
    "                self.rnn = getattr(torch.nn, self.rnn_type)(rnn_input_dim, hidden_size,\n",
    "                                                            num_layers=self.nlayers,\n",
    "                                                            bidirectional=True)\n",
    "            else:\n",
    "                self.rnn = getattr(torch.nn, self.rnn_type)(rnn_input_dim, hidden_size,\n",
    "                                                            num_layers=self.nlayers,\n",
    "                                                            dropout=0.5,\n",
    "                                                            bidirectional=True)\n",
    "\n",
    "        # final linear map to tag space\n",
    "        if self.use_rnn:\n",
    "            self.linear = torch.nn.Linear(hidden_size * 2, len(tag_dictionary))\n",
    "        else:\n",
    "            self.linear = torch.nn.Linear(self.embeddings.embedding_length, len(tag_dictionary))\n",
    "\n",
    "        if self.use_crf:\n",
    "            self.transitions = torch.nn.Parameter(\n",
    "                torch.randn(self.tagset_size, self.tagset_size))\n",
    "            self.transitions.data[self.tag_dictionary.get_idx_for_item(START_TAG), :] = -10000\n",
    "            self.transitions.data[:, self.tag_dictionary.get_idx_for_item(STOP_TAG)] = -10000\n",
    "\n",
    "        #if torch.cuda.is_available():\n",
    "        #    self.cuda()\n",
    "\n",
    "    def save(self, model_file: str):\n",
    "        model_state = {\n",
    "            'state_dict': self.state_dict(),\n",
    "            'embeddings': self.embeddings,\n",
    "            'hidden_size': self.hidden_size,\n",
    "            'tag_dictionary': self.tag_dictionary,\n",
    "            'tag_type': self.tag_type,\n",
    "            'use_crf': self.use_crf,\n",
    "            'use_rnn': self.use_rnn,\n",
    "            'rnn_layers': self.rnn_layers,\n",
    "            'use_word_dropout': self.use_word_dropout,\n",
    "            'use_locked_dropout': self.use_locked_dropout,\n",
    "        }\n",
    "\n",
    "        torch.save(model_state, model_file, pickle_protocol=4)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_file(cls, model_file):\n",
    "        # suppress torch warnings:\n",
    "        # https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            state = torch.load(model_file, map_location={'cuda:0': 'cpu'})\n",
    "\n",
    "        use_dropout = 0.0 if not 'use_dropout' in state.keys() else state['use_dropout']\n",
    "        use_word_dropout = 0.0 if not 'use_word_dropout' in state.keys() else state['use_word_dropout']\n",
    "        use_locked_dropout = 0.0 if not 'use_locked_dropout' in state.keys() else state['use_locked_dropout']\n",
    "\n",
    "        model = SequenceTagger(\n",
    "            hidden_size=state['hidden_size'],\n",
    "            embeddings=state['embeddings'],\n",
    "            tag_dictionary=state['tag_dictionary'],\n",
    "            tag_type=state['tag_type'],\n",
    "            use_crf=state['use_crf'],\n",
    "            use_rnn=state['use_rnn'],\n",
    "            rnn_layers=state['rnn_layers'],\n",
    "            use_dropout=use_dropout,\n",
    "            use_word_dropout=use_word_dropout,\n",
    "            use_locked_dropout=use_locked_dropout,\n",
    "        )\n",
    "\n",
    "        model.load_state_dict(state['state_dict'])\n",
    "        model.eval()\n",
    "        #if torch.cuda.is_available():\n",
    "        #    model = model.cuda()\n",
    "        return model\n",
    "\n",
    "    def forward(self, sentences: List[Sentence]):\n",
    "\n",
    "        self.zero_grad()\n",
    "\n",
    "        self.embeddings.embed(sentences)\n",
    "\n",
    "        # first, sort sentences by number of tokens\n",
    "        sentences.sort(key=lambda x: len(x), reverse=True)\n",
    "        longest_token_sequence_in_batch: int = len(sentences[0])\n",
    "\n",
    "        lengths: List[int] = [len(sentence.tokens) for sentence in sentences]\n",
    "        tag_list: List = []\n",
    "\n",
    "        # initialize zero-padded word embeddings tensor\n",
    "        sentence_tensor = torch.zeros([len(sentences),\n",
    "                                       longest_token_sequence_in_batch,\n",
    "                                       self.embeddings.embedding_length],\n",
    "                                      dtype=torch.float)\n",
    "\n",
    "        for s_id, sentence in enumerate(sentences):\n",
    "\n",
    "            # fill values with word embeddings\n",
    "            sentence_tensor[s_id][:len(sentence)] = torch.cat([token.get_embedding().unsqueeze(0)\n",
    "                                                               for token in sentence], 0)\n",
    "\n",
    "            # get the tags in this sentence\n",
    "            tag_idx: List[int] = [self.tag_dictionary.get_idx_for_item(token.get_tag(self.tag_type).value)\n",
    "                                  for token in sentence]\n",
    "            # add tags as tensor\n",
    "            #if torch.cuda.is_available():\n",
    "            #    tag_list.append(torch.cuda.LongTensor(tag_idx))\n",
    "            #else:\n",
    "            if True:\n",
    "                tag_list.append(torch.LongTensor(tag_idx))\n",
    "\n",
    "        sentence_tensor = sentence_tensor.transpose_(0, 1)\n",
    "        #if torch.cuda.is_available():\n",
    "        #    sentence_tensor = sentence_tensor.cuda()\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # FF PART\n",
    "        # --------------------------------------------------------------------\n",
    "        if self.use_dropout > 0.0:\n",
    "            sentence_tensor = self.dropout(sentence_tensor)\n",
    "        if self.use_word_dropout > 0.0:\n",
    "            sentence_tensor = self.word_dropout(sentence_tensor)\n",
    "        if self.use_locked_dropout > 0.0:\n",
    "            sentence_tensor = self.locked_dropout(sentence_tensor)\n",
    "\n",
    "        if self.relearn_embeddings:\n",
    "            sentence_tensor = self.embedding2nn(sentence_tensor)\n",
    "\n",
    "        if self.use_rnn:\n",
    "            packed = torch.nn.utils.rnn.pack_padded_sequence(sentence_tensor, lengths)\n",
    "\n",
    "            rnn_output, hidden = self.rnn(packed)\n",
    "\n",
    "            sentence_tensor, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(rnn_output)\n",
    "\n",
    "            if self.use_dropout > 0.0:\n",
    "                sentence_tensor = self.dropout(sentence_tensor)\n",
    "            # word dropout only before LSTM - TODO: more experimentation needed\n",
    "            # if self.use_word_dropout > 0.0:\n",
    "            #     sentence_tensor = self.word_dropout(sentence_tensor)\n",
    "            if self.use_locked_dropout > 0.0:\n",
    "                sentence_tensor = self.locked_dropout(sentence_tensor)\n",
    "\n",
    "        features = self.linear(sentence_tensor)\n",
    "\n",
    "        return features.transpose_(0, 1), lengths, tag_list\n",
    "\n",
    "    def _score_sentence(self, feats, tags, lens_):\n",
    "\n",
    "        if False:#torch.cuda.is_available():\n",
    "            start = torch.cuda.LongTensor([\n",
    "                self.tag_dictionary.get_idx_for_item(START_TAG)\n",
    "            ])\n",
    "            start = start[None, :].repeat(tags.shape[0], 1)\n",
    "\n",
    "            stop = torch.cuda.LongTensor([\n",
    "                self.tag_dictionary.get_idx_for_item(STOP_TAG)\n",
    "            ])\n",
    "            stop = stop[None, :].repeat(tags.shape[0], 1)\n",
    "\n",
    "            pad_start_tags = \\\n",
    "                torch.cat([start, tags], 1)\n",
    "            pad_stop_tags = \\\n",
    "                torch.cat([tags, stop], 1)\n",
    "        else:\n",
    "            start = torch.LongTensor([\n",
    "                self.tag_dictionary.get_idx_for_item(START_TAG)\n",
    "            ])\n",
    "            start = start[None, :].repeat(tags.shape[0], 1)\n",
    "\n",
    "            stop = torch.LongTensor([\n",
    "                self.tag_dictionary.get_idx_for_item(STOP_TAG)\n",
    "            ])\n",
    "\n",
    "            stop = stop[None, :].repeat(tags.shape[0], 1)\n",
    "\n",
    "            pad_start_tags = torch.cat([start, tags], 1)\n",
    "            pad_stop_tags = torch.cat([tags, stop], 1)\n",
    "\n",
    "        for i in range(len(lens_)):\n",
    "            pad_stop_tags[i, lens_[i]:] = \\\n",
    "                self.tag_dictionary.get_idx_for_item(STOP_TAG)\n",
    "\n",
    "        score = torch.FloatTensor(feats.shape[0])\n",
    "        #if torch.cuda.is_available():\n",
    "        #    score = score.cuda()\n",
    "\n",
    "        for i in range(feats.shape[0]):\n",
    "            r = torch.LongTensor(range(lens_[i]))\n",
    "            #if torch.cuda.is_available():\n",
    "            #    r = r.cuda()\n",
    "\n",
    "            score[i] = \\\n",
    "                torch.sum(\n",
    "                    self.transitions[pad_stop_tags[i, :lens_[i] + 1], pad_start_tags[i, :lens_[i] + 1]]\n",
    "                ) + \\\n",
    "                torch.sum(feats[i, r, tags[i, :lens_[i]]])\n",
    "\n",
    "        return score\n",
    "\n",
    "    def viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "        backscores = []\n",
    "\n",
    "        init_vvars = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
    "        init_vvars[0][self.tag_dictionary.get_idx_for_item(START_TAG)] = 0\n",
    "        forward_var = autograd.Variable(init_vvars)\n",
    "        #if torch.cuda.is_available():\n",
    "        #    forward_var = forward_var.cuda()\n",
    "\n",
    "        import torch.nn.functional as F\n",
    "        for feat in feats:\n",
    "            next_tag_var = forward_var.view(1, -1).expand(self.tagset_size, self.tagset_size) + self.transitions\n",
    "            _, bptrs_t = torch.max(next_tag_var, dim=1)\n",
    "            bptrs_t = bptrs_t.squeeze().data.cpu().numpy()\n",
    "            next_tag_var = next_tag_var.data.cpu().numpy()\n",
    "            viterbivars_t = next_tag_var[range(len(bptrs_t)), bptrs_t]\n",
    "            viterbivars_t = autograd.Variable(torch.FloatTensor(viterbivars_t))\n",
    "            #if torch.cuda.is_available():\n",
    "            #    viterbivars_t = viterbivars_t.cuda()\n",
    "            forward_var = viterbivars_t + feat\n",
    "            backscores.append(forward_var)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        terminal_var = forward_var + self.transitions[self.tag_dictionary.get_idx_for_item(STOP_TAG)]\n",
    "        terminal_var.data[self.tag_dictionary.get_idx_for_item(STOP_TAG)] = -10000.\n",
    "        terminal_var.data[self.tag_dictionary.get_idx_for_item(START_TAG)] = -10000.\n",
    "        best_tag_id = argmax(terminal_var.unsqueeze(0))\n",
    "\n",
    "        best_path = [best_tag_id]\n",
    "\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "\n",
    "        best_scores = []\n",
    "        for backscore in backscores:\n",
    "            softmax = F.softmax(backscore, dim=0)\n",
    "            _, idx = torch.max(backscore, 0)\n",
    "            prediction = idx.item()\n",
    "            best_scores.append(softmax[prediction].item())\n",
    "\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_dictionary.get_idx_for_item(START_TAG)\n",
    "        best_path.reverse()\n",
    "        return best_scores, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentences: List[Sentence]):\n",
    "        features, lengths, tags = self.forward(sentences)\n",
    "\n",
    "        if self.use_crf:\n",
    "\n",
    "            # pad tags if using batch-CRF decoder\n",
    "            if False:#torch.cuda.is_available():\n",
    "                tags, _ = pad_tensors(tags, torch.cuda.LongTensor)\n",
    "            else:\n",
    "                tags, _ = pad_tensors(tags, torch.LongTensor)\n",
    "\n",
    "            forward_score = self._forward_alg(features, lengths)\n",
    "            gold_score = self._score_sentence(features, tags, lengths)\n",
    "\n",
    "            score = forward_score - gold_score\n",
    "\n",
    "            return score.sum()\n",
    "\n",
    "        else:\n",
    "\n",
    "            score = 0\n",
    "            for sentence_feats, sentence_tags, sentence_length in zip(features, tags, lengths):\n",
    "                sentence_feats = sentence_feats[:sentence_length]\n",
    "\n",
    "                if False:#torch.cuda.is_available():\n",
    "                    tag_tensor = autograd.Variable(torch.cuda.LongTensor(sentence_tags))\n",
    "                else:\n",
    "                    tag_tensor = autograd.Variable(torch.LongTensor(sentence_tags))\n",
    "                score += torch.nn.functional.cross_entropy(sentence_feats, tag_tensor)\n",
    "\n",
    "            return score\n",
    "\n",
    "    def _forward_alg(self, feats, lens_):\n",
    "\n",
    "        init_alphas = torch.Tensor(self.tagset_size).fill_(-10000.)\n",
    "        init_alphas[self.tag_dictionary.get_idx_for_item(START_TAG)] = 0.\n",
    "\n",
    "        forward_var = torch.FloatTensor(\n",
    "            feats.shape[0],\n",
    "            feats.shape[1] + 1,\n",
    "            feats.shape[2],\n",
    "        ).fill_(0)\n",
    "\n",
    "        forward_var[:, 0, :] = init_alphas[None, :].repeat(feats.shape[0], 1)\n",
    "\n",
    "        #if torch.cuda.is_available():\n",
    "        #    forward_var = forward_var.cuda()\n",
    "\n",
    "        transitions = self.transitions.view(\n",
    "            1,\n",
    "            self.transitions.shape[0],\n",
    "            self.transitions.shape[1],\n",
    "        ).repeat(feats.shape[0], 1, 1)\n",
    "\n",
    "        for i in range(feats.shape[1]):\n",
    "            emit_score = feats[:, i, :]\n",
    "\n",
    "            tag_var = \\\n",
    "                emit_score[:, :, None].repeat(1, 1, transitions.shape[2]) + \\\n",
    "                transitions + \\\n",
    "                forward_var[:, i, :][:, :, None].repeat(1, 1, transitions.shape[2]).transpose(2, 1)\n",
    "\n",
    "            max_tag_var, _ = torch.max(tag_var, dim=2)\n",
    "\n",
    "            tag_var = tag_var - \\\n",
    "                      max_tag_var[:, :, None].repeat(1, 1, transitions.shape[2])\n",
    "\n",
    "            agg_ = torch.log(torch.sum(torch.exp(tag_var), dim=2))\n",
    "\n",
    "            cloned = forward_var.clone()\n",
    "            cloned[:, i + 1, :] = max_tag_var + agg_\n",
    "\n",
    "            forward_var = cloned\n",
    "\n",
    "        forward_var = forward_var[range(forward_var.shape[0]), lens_, :]\n",
    "\n",
    "        terminal_var = forward_var + \\\n",
    "                       self.transitions[self.tag_dictionary.get_idx_for_item(STOP_TAG)][None, :].repeat(\n",
    "                           forward_var.shape[0], 1)\n",
    "\n",
    "        alpha = log_sum_exp_batch(terminal_var)\n",
    "\n",
    "        return alpha\n",
    "\n",
    "    def predict(self, sentences: Union[List[Sentence], Sentence], mini_batch_size=32) -> List[Sentence]:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if type(sentences) is Sentence:\n",
    "                sentences = [sentences]\n",
    "\n",
    "            filtered_sentences = self._filter_empty_sentences(sentences)\n",
    "\n",
    "            # remove previous embeddings\n",
    "            clear_embeddings(filtered_sentences, also_clear_word_embeddings=True)\n",
    "\n",
    "            # make mini-batches\n",
    "            batches = [filtered_sentences[x:x + mini_batch_size] for x in\n",
    "                       range(0, len(filtered_sentences), mini_batch_size)]\n",
    "\n",
    "            for batch in batches:\n",
    "                scores, predicted_ids = self._predict_scores_batch(batch)\n",
    "                all_tokens = []\n",
    "                for sentence in batch:\n",
    "                    all_tokens.extend(sentence.tokens)\n",
    "\n",
    "                for (token, score, predicted_id) in zip(all_tokens, scores, predicted_ids):\n",
    "                    token: Token = token\n",
    "                    # get the predicted tag\n",
    "                    predicted_tag = self.tag_dictionary.get_item_for_index(predicted_id)\n",
    "                    token.add_tag(self.tag_type, predicted_tag, score)\n",
    "\n",
    "            return sentences\n",
    "\n",
    "    def _predict_scores_batch(self, sentences: List[Sentence]):\n",
    "        feature, lengths, tags = self.forward(sentences)\n",
    "\n",
    "        all_confidences = []\n",
    "        all_tags_seqs = []\n",
    "\n",
    "        for feats, length in zip(feature, lengths):\n",
    "\n",
    "            if self.use_crf:\n",
    "                confidences, tag_seq = self.viterbi_decode(feats[:length])\n",
    "            else:\n",
    "                import torch.nn.functional as F\n",
    "\n",
    "                tag_seq = []\n",
    "                confidences = []\n",
    "                for backscore in feats[:length]:\n",
    "                    softmax = F.softmax(backscore, dim=0)\n",
    "                    _, idx = torch.max(backscore, 0)\n",
    "                    prediction = idx.item()\n",
    "                    tag_seq.append(prediction)\n",
    "                    confidences.append(softmax[prediction].item())\n",
    "\n",
    "            all_tags_seqs.extend(tag_seq)\n",
    "            all_confidences.extend(confidences)\n",
    "\n",
    "        return all_confidences, all_tags_seqs\n",
    "\n",
    "    @staticmethod\n",
    "    def _filter_empty_sentences(sentences: List[Sentence]) -> List[Sentence]:\n",
    "        filtered_sentences = [sentence for sentence in sentences if sentence.tokens]\n",
    "        if len(sentences) != len(filtered_sentences):\n",
    "            log.warning('Ignore {} sentence(s) with no tokens.'.format(len(sentences) - len(filtered_sentences)))\n",
    "        return filtered_sentences\n",
    "\n",
    "    @staticmethod\n",
    "    def load(model: str):\n",
    "        model_file = None\n",
    "        aws_resource_path = 'https://s3.eu-central-1.amazonaws.com/alan-nlp/resources/models-v0.2'\n",
    "        if model == \"JNLPBA\":\n",
    "            model_file = \"resources/taggers/pubmed/+JNLPBA/final-model.pt\"\n",
    "        if model == \"BC2GM\":\n",
    "            model_file = \"resources/taggers/pubmed/+BC2GM/final-model.pt\"\n",
    "        if model == \"BC5CDR\":\n",
    "            model_file = \"resources/taggers/pubmed/+BC5CDR/final-model.pt\"\n",
    "        if model == 'protocol':\n",
    "            model_file = current_path + \"/resources/taggers/c4r/final-model.pt\"\t    \n",
    "        if model.lower() == \"medner\":\n",
    "            model_file = current_path + \"/resources/taggers/medner/final-model.pt\"\n",
    "        if model.lower() == \"medner_big\":\n",
    "            model_file = \"resources/taggers/result/elmo+flair+bigcorpus/final-model.pt\"\n",
    "        if model.lower() == 'ner':\n",
    "            base_path = '/'.join([aws_resource_path,\n",
    "                                  'NER-conll03--h256-l1-b32-%2Bglove%2Bnews-forward%2Bnews-backward--v0.2',\n",
    "                                  'en-ner-conll03-v0.2.pt'])\n",
    "            model_file = cached_path(base_path, cache_dir='models')\n",
    "\n",
    "        if model.lower() == 'ner-fast':\n",
    "            base_path = '/'.join([aws_resource_path,\n",
    "                                  'NER-conll03--h256-l1-b32-experimental--fast-v0.2',\n",
    "                                  'en-ner-fast-conll03-v0.2.pt'])\n",
    "            model_file = cached_path(base_path, cache_dir='models')\n",
    "\n",
    "        if model.lower() == 'ner-ontonotes':\n",
    "            base_path = '/'.join([aws_resource_path,\n",
    "                                  'NER-ontoner--h256-l1-b32-%2Bcrawl%2Bnews-forward%2Bnews-backward--v0.2',\n",
    "                                  'en-ner-ontonotes-v0.3.pt'])\n",
    "            model_file = cached_path(base_path, cache_dir='models')\n",
    "\n",
    "        if model.lower() == 'ner-ontonotes-fast':\n",
    "            base_path = '/'.join([aws_resource_path,\n",
    "                                  'NER-ontoner--h256-l1-b32-%2Bcrawl%2Bnews-forward-fast%2Bnews-backward-fast--v0.2',\n",
    "                                  'en-ner-ontonotes-fast-v0.3.pt'])\n",
    "            model_file = cached_path(base_path, cache_dir='models')\n",
    "\n",
    "        if model.lower() == 'pos':\n",
    "            base_path = '/'.join([aws_resource_path,\n",
    "                                  'POS-ontonotes--h256-l1-b32-%2Bmix-forward%2Bmix-backward--v0.2',\n",
    "                                  'en-pos-ontonotes-v0.2.pt'])\n",
    "            model_file = cached_path(base_path, cache_dir='models')\n",
    "\n",
    "        if model.lower() == 'pos-fast':\n",
    "            base_path = '/'.join([aws_resource_path,\n",
    "                                  'POS-ontonotes--h256-l1-b32-%2Bnews-forward-fast%2Bnews-backward-fast--v0.2',\n",
    "                                  'en-pos-ontonotes-fast-v0.2.pt'])\n",
    "            model_file = cached_path(base_path, cache_dir='models')\n",
    "\n",
    "        if model.lower() == 'frame':\n",
    "            base_path = '/'.join([aws_resource_path,\n",
    "                                  'FRAME-conll12--h256-l1-b8-%2Bnews%2Bnews-forward%2Bnews-backward--v0.2',\n",
    "                                  'en-frame-ontonotes-v0.2.pt'])\n",
    "            model_file = cached_path(base_path, cache_dir='models')\n",
    "\n",
    "        if model.lower() == 'frame-fast':\n",
    "            base_path = '/'.join([aws_resource_path,\n",
    "                                  'FRAME-conll12--h256-l1-b8-%2Bnews%2Bnews-forward-fast%2Bnews-backward-fast--v0.2',\n",
    "                                  'en-frame-ontonotes-fast-v0.2.pt'])\n",
    "            model_file = cached_path(base_path, cache_dir='models')\n",
    "\n",
    "        if model.lower() == 'chunk':\n",
    "            base_path = '/'.join([aws_resource_path,\n",
    "                                  'NP-conll2000--h256-l1-b32-%2Bnews-forward%2Bnews-backward--v0.2',\n",
    "                                  'en-chunk-conll2000-v0.2.pt'])\n",
    "            model_file = cached_path(base_path, cache_dir='models')\n",
    "\n",
    "        if model.lower() == 'chunk-fast':\n",
    "            base_path = '/'.join([aws_resource_path,\n",
    "                                  'NP-conll2000--h256-l1-b32-%2Bnews-forward-fast%2Bnews-backward-fast--v0.2',\n",
    "                                  'en-chunk-conll2000-fast-v0.2.pt'])\n",
    "            model_file = cached_path(base_path, cache_dir='models')\n",
    "\n",
    "        if model.lower() == 'de-pos':\n",
    "            base_path = '/'.join([aws_resource_path,\n",
    "                                  'UPOS-udgerman--h256-l1-b8-%2Bgerman-forward%2Bgerman-backward--v0.2',\n",
    "                                  'de-pos-ud-v0.2.pt'])\n",
    "            model_file = cached_path(base_path, cache_dir='models')\n",
    "\n",
    "        if model.lower() == 'de-ner':\n",
    "            base_path = '/'.join([aws_resource_path,\n",
    "                                  'NER-conll03ger--h256-l1-b32-%2Bde-fasttext%2Bgerman-forward%2Bgerman-backward--v0.2',\n",
    "                                  'de-ner-conll03-v0.3.pt'])\n",
    "            model_file = cached_path(base_path, cache_dir='models')\n",
    "\n",
    "        if model.lower() == 'de-ner-germeval':\n",
    "            base_path = '/'.join([aws_resource_path,\n",
    "                                  'NER-germeval--h256-l1-b32-%2Bde-fasttext%2Bgerman-forward%2Bgerman-backward--v0.2',\n",
    "                                  'de-ner-germeval-v0.3.pt'])\n",
    "            model_file = cached_path(base_path, cache_dir='models')\n",
    "\n",
    "        if model_file is not None:\n",
    "            tagger: SequenceTagger = SequenceTagger.load_from_file(model_file)\n",
    "            return tagger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "from typing import List, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#import flair.embeddings\n",
    "#from flair.data import Dictionary, Sentence, Label\n",
    "#from flair.training_utils import convert_labels_to_one_hot, clear_embeddings\n",
    "\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Text Classification Model\n",
    "    The model takes word embeddings, puts them into an LSTM to obtain a text representation, and puts the\n",
    "    text representation in the end into a linear layer to get the actual class label.\n",
    "    The model can handle single and multi class data sets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 document_embeddings: flair.embeddings.DocumentEmbeddings,\n",
    "                 label_dictionary: Dictionary,\n",
    "                 multi_label: bool):\n",
    "\n",
    "        super(TextClassifier, self).__init__()\n",
    "\n",
    "        self.document_embeddings = document_embeddings\n",
    "        self.label_dictionary: Dictionary = label_dictionary\n",
    "        self.multi_label = multi_label\n",
    "\n",
    "        self.document_embeddings: flair.embeddings.DocumentLSTMEmbeddings = document_embeddings\n",
    "\n",
    "        self.decoder = nn.Linear(self.document_embeddings.embedding_length, len(self.label_dictionary))\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "        if multi_label:\n",
    "            self.loss_function = nn.BCELoss()\n",
    "        else:\n",
    "            self.loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "        # auto-spawn on GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.decoder.weight)\n",
    "\n",
    "    def forward(self, sentences) -> List[List[float]]:\n",
    "        self.document_embeddings.embed(sentences)\n",
    "\n",
    "        text_embedding_list = [sentence.get_embedding() for sentence in sentences]\n",
    "        text_embedding_tensor = torch.cat(text_embedding_list, 0)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            text_embedding_tensor = text_embedding_tensor.cuda()\n",
    "\n",
    "        label_scores = self.decoder(text_embedding_tensor)\n",
    "\n",
    "        return label_scores\n",
    "\n",
    "    def save(self, model_file: str):\n",
    "        \"\"\"\n",
    "        Saves the current model to the provided file.\n",
    "        :param model_file: the model file\n",
    "        \"\"\"\n",
    "        model_state = {\n",
    "            'state_dict': self.state_dict(),\n",
    "            'document_embeddings': self.document_embeddings,\n",
    "            'label_dictionary': self.label_dictionary,\n",
    "            'multi_label': self.multi_label,\n",
    "        }\n",
    "        torch.save(model_state, model_file, pickle_protocol=4)\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_file(cls, model_file):\n",
    "        \"\"\"\n",
    "        Loads the model from the given file.\n",
    "        :param model_file: the model file\n",
    "        :return: the loaded text classifier model\n",
    "        \"\"\"\n",
    "\n",
    "        # ATTENTION: suppressing torch serialization warnings. This needs to be taken out once we sort out recursive\n",
    "        # serialization of torch objects\n",
    "        # https://docs.python.org/3/library/warnings.html#temporarily-suppressing-warnings\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            if torch.cuda.is_available():\n",
    "                state = torch.load(model_file)\n",
    "            else:\n",
    "                state = torch.load(model_file, map_location={'cuda:0': 'cpu'})\n",
    "\n",
    "        model = TextClassifier(\n",
    "            document_embeddings=state['document_embeddings'],\n",
    "            label_dictionary=state['label_dictionary'],\n",
    "            multi_label=state['multi_label']\n",
    "        )\n",
    "\n",
    "        model.load_state_dict(state['state_dict'])\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    def predict(self, sentences: Union[Sentence, List[Sentence]], mini_batch_size: int = 32) -> List[Sentence]:\n",
    "        \"\"\"\n",
    "        Predicts the class labels for the given sentences. The labels are directly added to the sentences.\n",
    "        :param sentences: list of sentences\n",
    "        :param mini_batch_size: mini batch size to use\n",
    "        :return: the list of sentences containing the labels\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            if type(sentences) is Sentence:\n",
    "                sentences = [sentences]\n",
    "\n",
    "            filtered_sentences = self._filter_empty_sentences(sentences)\n",
    "\n",
    "            batches = [filtered_sentences[x:x + mini_batch_size] for x in range(0, len(filtered_sentences), mini_batch_size)]\n",
    "\n",
    "            for batch in batches:\n",
    "                scores = self.forward(batch)\n",
    "                predicted_labels = self.obtain_labels(scores)\n",
    "\n",
    "                for (sentence, labels) in zip(batch, predicted_labels):\n",
    "                    sentence.labels = labels\n",
    "\n",
    "                clear_embeddings(batch)\n",
    "\n",
    "            return sentences\n",
    "\n",
    "    @staticmethod\n",
    "    def _filter_empty_sentences(sentences: List[Sentence]) -> List[Sentence]:\n",
    "        filtered_sentences = [sentence for sentence in sentences if sentence.tokens]\n",
    "        if len(sentences) != len(filtered_sentences):\n",
    "            log.warning('Ignore {} sentence(s) with no tokens.'.format(len(sentences) - len(filtered_sentences)))\n",
    "        return filtered_sentences\n",
    "\n",
    "    def calculate_loss(self, scores: List[List[float]], sentences: List[Sentence]) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the loss.\n",
    "        :param scores: the prediction scores from the model\n",
    "        :param sentences: list of sentences\n",
    "        :return: loss value\n",
    "        \"\"\"\n",
    "        if self.multi_label:\n",
    "            return self._calculate_multi_label_loss(scores, sentences)\n",
    "\n",
    "        return self._calculate_single_label_loss(scores, sentences)\n",
    "\n",
    "    def obtain_labels(self, scores: List[List[float]]) -> List[List[Label]]:\n",
    "        \"\"\"\n",
    "        Predicts the labels of sentences.\n",
    "        :param scores: the prediction scores from the model\n",
    "        :return: list of predicted labels\n",
    "        \"\"\"\n",
    "\n",
    "        if self.multi_label:\n",
    "            return [self._get_multi_label(s) for s in scores]\n",
    "\n",
    "        return [self._get_single_label(s) for s in scores]\n",
    "\n",
    "    def _get_multi_label(self, label_scores) -> List[Label]:\n",
    "        labels = []\n",
    "\n",
    "        sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "        results = list(map(lambda x: sigmoid(x), label_scores))\n",
    "        for idx, conf in enumerate(results):\n",
    "            if conf > 0.5:\n",
    "                label = self.label_dictionary.get_item_for_index(idx)\n",
    "                labels.append(Label(label, conf.item()))\n",
    "\n",
    "        return labels\n",
    "\n",
    "    def _get_single_label(self, label_scores) -> List[Label]:\n",
    "        conf, idx = torch.max(label_scores, 0)\n",
    "        label = self.label_dictionary.get_item_for_index(idx.item())\n",
    "\n",
    "        return [Label(label, conf.item())]\n",
    "\n",
    "    def _calculate_multi_label_loss(self, label_scores, sentences: List[Sentence]) -> float:\n",
    "        sigmoid = nn.Sigmoid()\n",
    "        return self.loss_function(sigmoid(label_scores), self._labels_to_one_hot(sentences))\n",
    "\n",
    "    def _calculate_single_label_loss(self, label_scores, sentences: List[Sentence]) -> float:\n",
    "        return self.loss_function(label_scores, self._labels_to_indices(sentences))\n",
    "\n",
    "    def _labels_to_one_hot(self, sentences: List[Sentence]):\n",
    "        label_list = [sentence.get_label_names() for sentence in sentences]\n",
    "        one_hot = convert_labels_to_one_hot(label_list, self.label_dictionary)\n",
    "        one_hot = [torch.FloatTensor(l).unsqueeze(0) for l in one_hot]\n",
    "        one_hot = torch.cat(one_hot, 0)\n",
    "        if torch.cuda.is_available():\n",
    "            one_hot = one_hot.cuda()\n",
    "        return one_hot\n",
    "\n",
    "    def _labels_to_indices(self, sentences: List[Sentence]):\n",
    "        indices = [\n",
    "            torch.LongTensor([self.label_dictionary.get_idx_for_item(label.value) for label in sentence.labels])\n",
    "            for sentence in sentences\n",
    "        ]\n",
    "\n",
    "        vec = torch.cat(indices, 0)\n",
    "        if torch.cuda.is_available():\n",
    "            vec = vec.cuda()\n",
    "\n",
    "        return vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load protocol model\n",
      ".ipynb_checkpoints\n",
      "requirements4.txt\n",
      "./pipeline/requirements4.txt\n",
      "--------\n",
      "opencv-python==4.1.0.25\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clear_embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-8e9cce06e1d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tagged_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_offset_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-f896ff1e69ea>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, sentences, mini_batch_size)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0;31m# remove previous embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             \u001b[0mclear_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malso_clear_word_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0;31m# make mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clear_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "#from flair.data import Sentence\n",
    "#from flair.models import SequenceTagger\n",
    "#import flair\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "flair.device = torch.device('cpu')\n",
    "tagger: SequenceTagger = SequenceTagger.load(\"ner\") # protocol ner\n",
    "\n",
    "#to be removed\n",
    "sys.argv[1] = './pipeline/'\n",
    "sys.argv[2] = './output/'\n",
    "print (\"load protocol model\")\n",
    "outfolder = sys.argv[2]\n",
    "for file in os.listdir(sys.argv[1]):\n",
    "    print(file)\n",
    "    if not file.endswith(\".txt\"):\n",
    "        continue\n",
    "    #print (file)\n",
    "    all_entities = []\n",
    "    offset = 0\n",
    "    print (sys.argv[1]+file) \n",
    "    for line in open(sys.argv[1]+file).readlines(): \n",
    "        print (\"--------\")\n",
    "        print (line.strip())\n",
    "        sentence: Sentence = Sentence(line.strip())\n",
    "        tagger.predict(sentence)\n",
    "        print (sentence.to_tagged_string())\n",
    "        entities = sentence.to_offset_tags(offset)\n",
    "        #print (entities)\n",
    "        new_entities = []\n",
    "        for ent in entities:\n",
    "            #if ent[3] not in [\"this study\", \"that study\", \"the disease\", \"the finding\", \"the study\", \"the analysis\", \"treatment\", \"test\", \"testing\", \"treatment\", \"symptoms\", \"Lilly\"]:\n",
    "            new_entities.append(ent)\n",
    "        offset += len(line)\n",
    "        #print(sentence.to_tagged_string())\n",
    "        all_entities += new_entities\n",
    "        #print (entities)\n",
    "    \n",
    "    filew = open(outfolder + file.replace(\".txt\", \".ann\"), \"w\")\n",
    "    filew2 = open(outfolder + file, \"w\")\n",
    "    txt = open(sys.argv[1]+file).read()\n",
    "    filew2.write(txt)\n",
    "    filew2.close\n",
    "    index = 1\n",
    "    for entity in all_entities: \n",
    "        fixed = False\n",
    "        if txt[int(entity[1]):int(entity[2])] != entity[3]:\n",
    "            for offset in [-5, -4, -3, -2, -1, 1, 2, 3,4,5]:\n",
    "                if (offset < 0 and int(entity[1]) >= abs(offset) and txt[int(entity[1])-offset:int(entity[2])-offset] == entity[3]) or (offset > 0 and int(entity[2]) <= len(txt)-offset and txt[int(entity[1])+offset:int(entity[2])+offset] == entity[3]):\n",
    "                    fixed = True\n",
    "                    entity[1] = str(int(entity[1]) + offset)\n",
    "                    entity[2] = str(int(entity[2]) + offset)\n",
    "                    \n",
    "            #if not fixed:\n",
    "            #    print (\"does not match found:\")\n",
    "            #    print (entity)\n",
    "            #    print (txt[int(entity[1]):int(entity[2])] )\n",
    "        else:\n",
    "            fixed = True\n",
    "        if fixed:\n",
    "            filew.write(\"T\"+str(index)+\"\\t\"+entity[0]+\" \"+entity[1]+\" \"+entity[2]+\"\\t\"+entity[3]+\"\\n\")\n",
    "        index += 1\n",
    "    filew.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
